<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Hidden Markov models – Stochastic Processes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./04_markov_continuous.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05_HMM.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hidden Markov models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./cover_spiral.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Stochastic Processes</a> 
        <div class="sidebar-tools-main">
    <a href="./Stochastic-Processes.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_markov_discrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Discrete-time Markov processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Poisson processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_markov_continuous.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Continuous-time Markov processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_HMM.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hidden Markov models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#mixture-models" id="toc-mixture-models" class="nav-link active" data-scroll-target="#mixture-models"><span class="header-section-number">5.1</span> Mixture models</a></li>
  <li><a href="#hidden-markov-models" id="toc-hidden-markov-models" class="nav-link" data-scroll-target="#hidden-markov-models"><span class="header-section-number">5.2</span> Hidden Markov models</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">5.2.1</span> Definition</a></li>
  <li><a href="#marginal-distribution" id="toc-marginal-distribution" class="nav-link" data-scroll-target="#marginal-distribution"><span class="header-section-number">5.2.2</span> Marginal distribution</a></li>
  <li><a href="#simulating-from-a-hidden-markov-model" id="toc-simulating-from-a-hidden-markov-model" class="nav-link" data-scroll-target="#simulating-from-a-hidden-markov-model"><span class="header-section-number">5.2.3</span> Simulating from a hidden Markov model</a></li>
  </ul></li>
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood"><span class="header-section-number">5.3</span> Likelihood</a>
  <ul class="collapse">
  <li><a href="#first-attempt" id="toc-first-attempt" class="nav-link" data-scroll-target="#first-attempt"><span class="header-section-number">5.3.1</span> First attempt</a></li>
  <li><a href="#second-attempt-forward-algorithm" id="toc-second-attempt-forward-algorithm" class="nav-link" data-scroll-target="#second-attempt-forward-algorithm"><span class="header-section-number">5.3.2</span> Second attempt: forward algorithm</a></li>
  </ul></li>
  <li><a href="#state-probabilities" id="toc-state-probabilities" class="nav-link" data-scroll-target="#state-probabilities"><span class="header-section-number">5.4</span> State probabilities</a></li>
  <li><a href="#some-examples" id="toc-some-examples" class="nav-link" data-scroll-target="#some-examples"><span class="header-section-number">5.5</span> Some examples</a>
  <ul class="collapse">
  <li><a href="#animal-telemetry" id="toc-animal-telemetry" class="nav-link" data-scroll-target="#animal-telemetry"><span class="header-section-number">5.5.1</span> Animal telemetry</a></li>
  <li><a href="#oil-price" id="toc-oil-price" class="nav-link" data-scroll-target="#oil-price"><span class="header-section-number">5.5.2</span> Oil price</a></li>
  </ul></li>
  <li><a href="#problems" id="toc-problems" class="nav-link" data-scroll-target="#problems"><span class="header-section-number">5.6</span> Problems</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hidden Markov models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Markov processes are a convenient approach to model temporal dependence while retaining (some) mathematical simplicity but, to estimate the Markov process from data, we need to observe that process directly. However, there are many situations where the process is observed only indirectly, i.e., our observations depend on something that can be described by a Markov process. There is a vast literature on models for such situations; they are called state-space models when the state space of the Markov process is continuous, and hidden Markov models when it is discrete. Hidden Markov models were developed more recently than other models covered in this course, but they are now widely-used in various areas of applications (finance, medicine, ecology, etc.).</p>
<section id="mixture-models" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="mixture-models"><span class="header-section-number">5.1</span> Mixture models</h2>
<p>We start with the description of mixture models, on which hidden Markov models build. A mixture model describes a random variable which can come from several different distributions, each with some probability.</p>
<p>Consider the random variable <span class="math inline">\(Z\)</span>, which follows one of <span class="math inline">\(K\)</span> distributions, with respective probability density (or mass) functions <span class="math inline">\(b_1, b_2, \dots, b_K\)</span>. For any <span class="math inline">\(k \in \{ 1, \dots, K \}\)</span>, we further assume that <span class="math inline">\(Z\)</span> follows the <span class="math inline">\(k^\text{th}\)</span> distribution (<span class="math inline">\(b_k\)</span>) with probability <span class="math inline">\(\pi_k\)</span>, where <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>.</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Throughout this chapter, we will consider random variables that can be either discrete or continuous. Rather than writing every equation twice, we will use generic notation that applies in both cases.<br>
</p>
<p>Specifically, we will use <span class="math inline">\(f(Z = z)\)</span> to represent either a probability (if <span class="math inline">\(Z\)</span> is discrete) or a probability density (if <span class="math inline">\(Z\)</span> is continuous).</p>
</div>
</div>
<p>The probability mass/density function of <span class="math inline">\(Z\)</span> under this model is a linear combination of the component functions, each weighted by the probability of the component: <span class="math display">\[
\begin{aligned}
    f(Z = z) &amp; = \sum_{k=1}^K f(Z = z \mid C = k) \times \Pr(C = k) \\
        &amp; = \sum_{k = 1}^K \pi_k b_k(z)
\end{aligned}
\]</span></p>
<p>Examples of mixture model with three components is shown in <a href="#fig-mix-model" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-mix-model</span></a>. For one of them, the <span class="math inline">\(b_k\)</span> are normal probability distribution functions; for the other, they are Poisson probability mass functions. In both cases, the mixture model has much more flexibility than a single distribution from that family.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mix-model" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mix-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-mix-model-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mix-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Example mixture models with three components <span class="math inline">\(p_1, p_2, p_3\)</span>, resulting in the probability distribution <span class="math inline">\(f_Z\)</span> shown in black. The component distributions are weighted by the probabilities <span class="math inline">\(\pi_1, \pi_2, \pi_3\)</span>. On the left, each component is a normal distribution; on the right, each component is a Poisson distribution.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Mixture models have been popular for model-based clustering. Consider <span class="math inline">\(n\)</span> observations <span class="math inline">\(z_1, \dots, z_n\)</span>, assumed to be realisations from <span class="math inline">\(n\)</span> independent random variables described by some mixture model. Various approaches have been developed to estimate parameters of the component distributions and the weight <span class="math inline">\(\pi_i\)</span> of each component, and to group the observations by “most likely component”.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 5.1
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider the distribution of flipper length from 344 penguins shown in <a href="#fig-penguins" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-penguins</span></a> (from the R package <code>palmerpenguins</code>). The distribution is clearly bimodal, and we suspect that data from two different species have been mixed up.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-penguins" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-penguins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-penguins-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-penguins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Histogram of flipper lengths in penguin data set.
</figcaption>
</figure>
</div>
</div>
</div>
<p>A Gaussian mixture model could be used to answer questions such as:</p>
<ol type="1">
<li>Which species does each data point belong to?</li>
<li>What is the distribution of flipper lengths for each species?</li>
<li>What is the overall distribution of flipper lengths for both species?<br>
</li>
</ol>
<p><a href="#fig-penguins2" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-penguins2</span></a> shows the two distributions that we would obtain from a Gaussian mixture model fitted to these data.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-penguins2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-penguins2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-penguins2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-penguins2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: Estimated mixture distributions for penguin flipper data.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
<p>Hidden Markov models can be viewed as dependent mixture models, i.e., where successive observations are not independent.</p>
</section>
<section id="hidden-markov-models" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="hidden-markov-models"><span class="header-section-number">5.2</span> Hidden Markov models</h2>
<section id="definition" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">5.2.1</span> Definition</h3>
<p>A hidden Markov model (HMM) consists of two stochastic processes, a state process <span class="math inline">\((X_n)\)</span>, and a state-dependent observation process <span class="math inline">\((Z_n)\)</span>. Both processes can in principle be continuous-valued, but we will focus on the case where <span class="math inline">\(X_n\)</span> is defined over a countable set <span class="math inline">\(\mathcal{S} \equiv \{ 0, 1, 2, \dots\}\)</span>. Time can also be discrete or continuous, and in this chapter we focus on discrete-time HMMs, with the index <span class="math inline">\(m\)</span> or <span class="math inline">\(n\)</span>. (We sometimes prefer <span class="math inline">\(m\)</span> because <span class="math inline">\(n\)</span> is traditionally used for the size of the data set, as we do when discussing the likelihood derivation below.)</p>
<p>HMMs are characterised by the following dependence assumptions:</p>
<ol type="1">
<li><p>The state process <span class="math inline">\((X_n)\)</span> is a Markov chain, such that <span class="math display">\[
\Pr(X_{n+1} \mid X_n, X_{n - 1}, \dots, X_0) = \Pr(X_{n+1} \mid X_n).
\]</span></p></li>
<li><p>The observation <span class="math inline">\(Z_n\)</span> is independent of past values of the process, conditional on the current state <span class="math inline">\(X_n\)</span>. That is, <span class="math display">\[
f(Z_n \mid Z_{n-1}, \dots, Z_0, X_{n}, \dots, X_0) = f(Z_n \mid X_n)
\]</span></p></li>
</ol>
<p>That is, <span class="math inline">\(Z_n\)</span> comes from a mixture model, where the mixture component active at time <span class="math inline">\(n\)</span> is given by <span class="math inline">\(X_n\)</span>. In most situations, the Markov chain is such that there is persistence in the state (i.e., the process tends to remain in the same state for several time steps), and this creates correlation between successive values of <span class="math inline">\(Z_n\)</span>.</p>
<p>A simulated realisation from an HMM where <span class="math inline">\(X_n \in \{ 0, 1, 2 \}\)</span> and <span class="math inline">\(Z_n \mid X_n = j \sim \text{Pois}(\lambda_j)\)</span> is shown in <a href="#fig-hmm-sim" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-hmm-sim</span></a>. It is clear that the dependence on <span class="math inline">\(X_n\)</span> induces autocorrelation in the observation process <span class="math inline">\((Z_n)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-hmm-sim" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hmm-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-hmm-sim-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hmm-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Simulation from a 3-state hidden Markov model with Poisson state-dependent distributions. The top panel shows the simulated state sequence, the bottom-left panel shows the simulated observation sequence, and the bottom-right panel shows the three Poisson distributions.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Terminology: What exactly is “hidden”?
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is called a <em>hidden</em> Markov model because we usually only have observations from the state-dependent process <span class="math inline">\((Z_n)\)</span>, but no direct observations from the state process <span class="math inline">\((X_n)\)</span>. The state process is therefore “hidden”, or “unobserved”, or “latent”. The problem is then to try to infer the dynamics of the hidden state process, based on the observations. This type of inference is very common in science, because it is often the case that we cannot directly observe the phenomenon of interest, e.g., because of measurement error.</p>
</div>
</div>
</section>
<section id="marginal-distribution" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="marginal-distribution"><span class="header-section-number">5.2.2</span> Marginal distribution</h3>
<p>The model for the observation process <span class="math inline">\((Z_t)\)</span> is defined through the conditional distribution of <span class="math inline">\(Z_t\)</span> given the state <span class="math inline">\(X_t\)</span>. We denote as <span class="math inline">\(b_k\)</span> the probability density/mass function of <span class="math inline">\(Z_t\)</span> in state <span class="math inline">\(k\)</span>, i.e., <span class="math inline">\(b_k(z) = f(Z_n = z \mid X_n = k)\)</span>. Let <span class="math inline">\(\boldsymbol{B}(z)\)</span> be the diagonal matrix with <span class="math inline">\(i^\text{th}\)</span> diagonal element <span class="math inline">\(b_i(z)\)</span>. Like in Chapter 2, let <span class="math inline">\(\boldsymbol{u}^{(n)} = (u^{(n)}_0, u_1^{(n)}, \dots)\)</span> be the probability distribution of <span class="math inline">\(X_n\)</span>, i.e., <span class="math inline">\(u_i^{(n)} = \Pr(X_n = i)\)</span>.</p>
<p>We are sometimes interested in the <em>marginal</em> distribution of the observation in a hidden Markov model, i.e., <span class="math inline">\(f(Z_n = z)\)</span> (not conditional on the state <span class="math inline">\(X_n\)</span>). By the law of total probability, we have <span class="math display">\[
\begin{aligned}
    f(Z_n = z) &amp; = \sum_{i \in \mathcal{S}} \Pr(X_n = i) f(Z_n = z \mid X_n = i) \\
        &amp; = \sum_{i \in \mathcal{S}} u_i^{(n)} b_i(z) \\
        &amp; = \boldsymbol{u}^{(n)} \boldsymbol{B}(z) \boldsymbol{1}^\intercal,
\end{aligned}
\]</span> where <span class="math inline">\(\boldsymbol{1}\)</span> is a (row) vector of ones. From Chapter 2, we know that the distribution of the state variable <span class="math inline">\(X_n\)</span> can be written in terms of its initial distribution <span class="math inline">\(\boldsymbol{u}^{(0)}\)</span> and transition probability matrix <span class="math inline">\(\boldsymbol{P}\)</span>, as <span class="math inline">\(\boldsymbol{u}^{(n)} = \boldsymbol{u}^{(0)} \boldsymbol{P}^n\)</span>. (This was a consequence of the Chapman-Kolmogorov equations.) Using this result, we find <span class="math display">\[
    f(Z_n = z) = \boldsymbol{u}^{(0)} \boldsymbol{P}^n \boldsymbol{B}(z) \boldsymbol{1}^\intercal.
\]</span></p>
</section>
<section id="simulating-from-a-hidden-markov-model" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="simulating-from-a-hidden-markov-model"><span class="header-section-number">5.2.3</span> Simulating from a hidden Markov model</h3>
<p>We use the dependence structure to define a simulation procedure. The hidden state process is simply a discrete-time Markov chain, so we can simulate it as described in Chapter 2. Once we have a simulated state sequence, we can simulate the observation process at each time step conditionally on the state.</p>
<ol type="1">
<li><p>Initialise <span class="math inline">\(X_0\)</span> based on the initial distribution <span class="math inline">\(\boldsymbol{u}^{(0)}\)</span>.</p></li>
<li><p>For <span class="math inline">\(n = 1, 2, \dots\)</span>, simulate <span class="math inline">\(X_n\)</span> conditionally on <span class="math inline">\(X_{n+1} = i\)</span> using the transition probabilities <span class="math inline">\(\{ \gamma_{ij} \}_{j \in \mathcal{S}}\)</span> (i.e., the <span class="math inline">\(i^\text{th}\)</span> row of the transition probability matrix).</p></li>
<li><p>For <span class="math inline">\(n = 0, 1, \dots\)</span>, simulate <span class="math inline">\(Z_n\)</span> conditionally on <span class="math inline">\(X_n = i\)</span> using the observation distribution <span class="math inline">\(b_i\)</span>.</p></li>
</ol>
<p>The following code shows an example over 100 time steps, for a 2-state hidden Markov model with normal state-dependent distributions, with model parameters <span class="math display">\[
\begin{aligned}
    &amp; \boldsymbol{u}^{(0)} = (0.5, 0.5) \\
    &amp; \boldsymbol{P} =
    \begin{pmatrix}
        0.9 &amp; 0.1 \\
        0.05 &amp; 0.95
    \end{pmatrix} \\
    &amp; Z_n \mid X_n = 0 \sim N(10, 3^2) \\
    &amp; Z_n \mid X_n = 1 \sim N(20, 2^2)
\end{aligned}
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">294</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.9</span>, <span class="fl">0.1</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>              <span class="fl">0.05</span> , <span class="fl">0.95</span>),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">20</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate state process</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="at">length =</span> n)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> u)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n) {</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    P_row <span class="ot">&lt;-</span> P[X[i<span class="dv">-1</span>] <span class="sc">+</span> <span class="dv">1</span>,]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    X[i] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> P_row)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate observation process</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="at">length =</span> n)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    Z[i] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mu[X[i] <span class="sc">+</span> <span class="dv">1</span>], <span class="at">sd =</span> sigma[X[i] <span class="sc">+</span> <span class="dv">1</span>])</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span>n, <span class="at">X =</span> X, <span class="at">Z =</span> Z), <span class="fu">aes</span>(time, Z)) <span class="sc">+</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">lty =</span> <span class="dv">2</span>, <span class="at">linewidth =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">col =</span> <span class="fu">factor</span>(X))) <span class="sc">+</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#00798c"</span>, <span class="st">"#d1495b"</span>), <span class="at">guide =</span> <span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="05_HMM_files/figure-html/sim-hmm-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="likelihood" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="likelihood"><span class="header-section-number">5.3</span> Likelihood</h2>
<p>Hidden Markov models are mostly interesting to applied statisticians and scientists (rather than probabilists), and so most of the related work has been on statistical inference. Given a sequence of observations, the main questions are usually:</p>
<ul>
<li>Can we estimate the state-dependent distributions <span class="math inline">\(b_k\)</span>?</li>
<li>Can we estimate the transition probabilities of the underlying Markov chain?</li>
<li>Can we infer the most likely value for the state process at each time step?</li>
</ul>
<p>An important step to answer these questions is to derive the likelihood function for this model, i.e., the joint probability (or probability density) of a sequence of observations. This is challenging because the model is specified in terms of random variables that are not observed (the states <span class="math inline">\(X_1, X_2, \dots\)</span>), and this section presents two methods to compute the likelihood. The mathematical derivations might seem a little tedious, but you will notice that we only use basic probability rules, and we take advantage of the dependence structure of the hidden Markov model to find the likelihood.</p>
<section id="first-attempt" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="first-attempt"><span class="header-section-number">5.3.1</span> First attempt</h3>
<p>We want to find the joint probability density/mass function of the random variables <span class="math inline">\(Z_0, Z_1, \dots, Z_n\)</span>, which we will denote as <span class="math inline">\(L = f(Z_0 = z_0, \dots, Z_n = z_n)\)</span> for convenience. When viewed as a function of the unknown model parameters, <span class="math inline">\(L\)</span> is the likelihood, and can be maximised numerically for estimation. We will also write <span class="math inline">\(\boldsymbol{Z}_{0:n} = \{ Z_0, \dots, Z_n \}\)</span> and similar notation for brevity.</p>
<p>By repeatedly applying the law of total probability, and leveraging the Markov property of <span class="math inline">\((X_n)\)</span>, we see that <span class="math display">\[
\begin{aligned}
    L &amp; = \sum_{X_n \in \mathcal{S}} f(\boldsymbol{Z}_{0:n} \mid X_n) \Pr(X_n) \\
        &amp; = \sum_{X_n \in \mathcal{S}} \sum_{X_{n-1} \in \mathcal{S}}
            f(\boldsymbol{Z}_{0:n} \mid X_{n-1}, X_n) \Pr(X_n \mid X_{n-1}) \Pr(X_{n-1}) \\
        &amp; = \sum_{X_n \in \mathcal{S}}
            \sum_{X_{n-1} \in \mathcal{S}}
            \sum_{X_{n-2} \in \mathcal{S}}
            f(\boldsymbol{Z}_{:n} \mid X_{n-2}, X_{n-1}, X_n) \\
        &amp; \qquad\qquad\times \Pr(X_n \mid X_{n-1}, X_{n-2})
            \Pr(X_{n-1} \mid X_{n-2})
            \Pr(X_{n-2}) \\
        &amp; = \sum_{X_n \in \mathcal{S}}
            \sum_{X_{n-1} \in \mathcal{S}}
            \sum_{X_{n-2} \in \mathcal{S}}
            f(\boldsymbol{Z}_{0:n} \mid X_{n-2}, X_{n-1}, X_n) \\
        &amp; \qquad\qquad\times \Pr(X_n \mid X_{n-1})
            \Pr(X_{n-1} \mid X_{n-2})
            \Pr(X_{n-2}) \\
        &amp; = \dots \\
        &amp; = \sum_{X_0 \in \mathcal{S}} \dots \sum_{X_n \in \mathcal{S}} \left\{
            f(\boldsymbol{Z}_{0:n} \mid X_{0:n}) \times
            \Pr(X_0) \times
            \prod_{m=1}^n \Pr(X_m \mid X_{m-1}) \right\}
\end{aligned}
\]</span></p>
<p>(Above, we are using a slight abuse of notation, so that the formula fits on a page. Take some time to think about it, and make sure you understand what the probabilities and <span class="math inline">\(f\)</span> refer to.)</p>
<p>Now, remember that the observations are conditionally independent given the states, so <span class="math display">\[
    f(\boldsymbol{Z}_{0:n} \mid X_{0:n}) = \prod_{m=0}^n f(Z_m \mid X_m)
\]</span></p>
<p>We now recognise that the likelihood can be written in terms of the state-dependent distributions, the initial distribution of the state process, and the transition probabilities. As before, we use the notation</p>
<ul>
<li><span class="math inline">\(b_k(z) = f(Z_m = z \mid X_m = k)\)</span>,</li>
<li><span class="math inline">\(P_{ij} = \Pr(X_{m+1} = j \mid X_m = i)\)</span>,</li>
<li><span class="math inline">\(u_i^{(m)} = \Pr(X_{m} = i)\)</span>.</li>
</ul>
<p>Then, the joint density of the observations is <span class="math display">\[
    L =
        \sum_{x_0 \in \mathcal{S}} \dots \sum_{x_n \in \mathcal{S}} \left\{
            u_{x_0}^{(0)} \prod_{m=0}^n b_{x_m}(z_m) \prod_{m=1}^n P_{x_{m-1},x_m}
        \right\}
\]</span></p>
<p>This is a relatively simple expression, which would in principle be straightforward to implement with a computer (for maximum likelihood estimation, for example). However, it is extremely computationally expensive, with <span class="math inline">\(\vert \mathcal{S} \vert^{n+1}\)</span> terms to sum, and often not a practical option. The challenge is to sum over all unobserved state sequences, because there are so many possible combinations. In the next section, we present an alternative approach to evaluating the likelihood, which uses matrix operations and offers an elegant solution to this problem.</p>
</section>
<section id="second-attempt-forward-algorithm" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="second-attempt-forward-algorithm"><span class="header-section-number">5.3.2</span> Second attempt: forward algorithm</h3>
<p>A more efficient algorithm to compute the joint probability of observations can be derived using so-called forward variables, or forward probabilities.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 5.1
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>forward probability</strong> <span class="math inline">\(\alpha_k^{(m)}\)</span> is defined as <span class="math display">\[
    \alpha_k^{(m)} = f(Z_0 = z_0, \dots, Z_m = z_m, X_m = k),
\]</span> for state <span class="math inline">\(k \in \mathcal{S}\)</span> and time <span class="math inline">\(m \in \{ 0, 1, 2, \dots\}\)</span>.</p>
</div>
</div>
<p>When the observation variables are continuous, <span class="math inline">\(\alpha_k^{(m)}\)</span> represents a probability density rather than a probability, but the term “forward probability” tends to be used loosely in both cases. In what follows, we denote as <span class="math inline">\(\boldsymbol{\alpha}^{(m)} = (\alpha_0^{(m)}, \alpha_1^{(m)}, \dots)\)</span> the vector of forward probabilities.</p>
<p>We would like to find an iterative method to compute the forward probabilities.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 5.1
</div>
</div>
<div class="callout-body-container callout-body">
<p>The forward probabilities satisfy <span class="math display">\[
    \boldsymbol\alpha^{(m)} = \boldsymbol\alpha^{(m-1)} \boldsymbol{PB}(z_m)
\]</span> where <span class="math inline">\(\boldsymbol\alpha^{(0)} = \boldsymbol{u}^{(0)} \boldsymbol{B}(z_0)\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>First, we notice that <span class="math display">\[
\begin{aligned}
\alpha_k^{(0)} &amp; = f(Z_0 = z_0, X_0 = k) \\
    &amp; = f(Z_0 = z_0 \mid X_0 = k) \Pr(X_0 = k) \\
    &amp; = b_k(z_0) u_k^{(0)}
\end{aligned}
\]</span> That is, the first vector of forward probabilities is obtained as <span class="math inline">\(\boldsymbol{\alpha}^{(0)} = \boldsymbol{u}^{(0)} \boldsymbol{B}(z_0)\)</span>.</p>
<p>Next, we use the dependence assumptions of the HMM to write <span class="math inline">\(\alpha^{(m)}\)</span> in terms of <span class="math inline">\(\alpha^{(m-1)}\)</span>: <span class="math display">\[
\begin{aligned}
    \alpha_k^{(m)} &amp; = f(Z_{0:m} = z_{0:m}, X_m = k) \\
        &amp; = \sum_{i \in \mathcal{S}} f(Z_{0:m} = z_{0:m}, X_m = k, X_{m-1} = i) &amp; \text{(a)}\\
        &amp; = \sum_{i \in \mathcal{S}} f(Z_m = z_m \mid X_m = k, X_{m-1} = i, Z_{0:(m-1)} = z_{0:(m-1)})\\
        &amp; \qquad\qquad \times f(X_m = k, X_{m-1} = i, Z_{0:(m-1)} = z_{0:(m-1)}) &amp; \text{(b)} \\
        &amp; = \sum_{i \in \mathcal{S}} f(Z_m = z_m \mid X_m = k) \\
        &amp; \qquad\qquad\times f(X_m = k \mid X_{m-1} = i, Z_{0:(m-1)} = z_{0:(m-1)}) \\
        &amp; \qquad\qquad\times f(X_{m-1} = i, Z_{0:(m-1)} = z_{0:(m-1)}) &amp; \text{(c)} \\
        &amp; = \sum_{i \in \mathcal{S}} b_k(z_m) P_{ik} \alpha_i^{(m - 1)}
\end{aligned}
\]</span> where (a) is the law of total probability, (b) is by definition of conditional probability, and (c) uses independence assumptions and conditional probability. Equivalently, in matrix notation, <span class="math display">\[
\boldsymbol{\alpha}^{(m)} = \boldsymbol{\alpha}^{(m-1)} \boldsymbol{P} \boldsymbol{B}(z_m).
\]</span></p>
</div>
</div>
</div>
<p>Applying this iteration from <span class="math inline">\(m = 1\)</span> to <span class="math inline">\(m = n\)</span> yields <span class="math display">\[
    \boldsymbol\alpha^{(n)} = \boldsymbol{u}^{(0)} \boldsymbol{B}(z_0) \boldsymbol{P} \boldsymbol{B}(z_1) \cdots \boldsymbol{P} \boldsymbol{B}(z_n).
\]</span></p>
<p>These computations are called the forward algorithm. There is a close link between the forward probabilities and the likelihood, and this gives us another approach to get the likelihood.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 5.2
</div>
</div>
<div class="callout-body-container callout-body">
<p>The likelihood of a hidden Markov model is given by <span class="math display">\[
    L = \boldsymbol{u}^{(0)} \boldsymbol{B}(z_0) \boldsymbol{P} \boldsymbol{B}(z_1) \dots \boldsymbol{P} \boldsymbol{B}(z_n) \boldsymbol{1}^\intercal
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using the law of total probability, <span class="math display">\[
\begin{aligned}
    L &amp; = f(Z_0 = z_0, \dots, Z_n = z_n) \\
        &amp; = \sum_{k \in \mathcal{S}} f(Z_0 = z_0, \dots, Z_n = z_n, X_n = k) \\
        &amp; = \sum_{k \in \mathcal{S}} \alpha_{k}^{(n)} \\
        &amp; = \boldsymbol{\alpha}^{(n)} \boldsymbol{1}^\intercal \\
        &amp; = \boldsymbol{u}^{(0)} \boldsymbol{B}(z_0) \boldsymbol{P} \boldsymbol{B}(z_1) \cdots \boldsymbol{P} \boldsymbol{B}(z_n) \boldsymbol{1}^\intercal
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<p>Remarkably, the number of operations required to evaluate the matrix product is much smaller than for the original nested sums. It is of the order of <span class="math inline">\(n \vert \mathcal{S} \vert^2\)</span>, and makes it possible to compute the likelihood in many situations. This efficient algorithm has greatly contributed to the popularity of hidden Markov models.</p>
<!-- A similar algorithm (called the "Viterbi algorithm", after its inventor) can be used to derive the most likely state sequence, given the observations and a set of estimated parameters. In many applications, this is the main object of inference. -->
</section>
</section>
<section id="state-probabilities" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="state-probabilities"><span class="header-section-number">5.4</span> State probabilities</h2>
<p>In the previous section, we computed the probability of a sequence of observations, marginalising over all possible state sequences. Another common problem is to determine what states are most likely to have given rise to a sequence of observations. There are multiple ways to approach this question, and here we focus on deriving the probability that each observation was generated by each state, <span class="math display">\[
\eta_k^{(m)} = \Pr(X_m = k \mid Z_0 = z_0, \dots, Z_n = z_n)
\]</span></p>
<p>To derive this probability, we first introduce another convenient quantity.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 5.2
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>backward probability</strong> <span class="math inline">\(\beta_k^{(m)}\)</span> is defined as <span class="math display">\[
    \beta_k^{(m)} = f(Z_{m+1} = z_{m+1}, \dots, Z_{n} = z_n \mid X_m = k)
\]</span> for state <span class="math inline">\(k \in \mathcal{S}\)</span> and time <span class="math inline">\(m \in \{ 0, 1, \dots, n-1 \}\)</span>.</p>
</div>
</div>
<p>In practice, the backward probabilities can be computed iteratively, similarly to the forward algorithm.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 5.3
</div>
</div>
<div class="callout-body-container callout-body">
<p>The backward probabilities satisfy <span class="math display">\[
    \boldsymbol\beta^{(m)} = \boldsymbol{PB}(z_{m+1}) \boldsymbol{\beta}^{(m+1)}
\]</span> where, by convention, <span class="math inline">\(\boldsymbol\beta^{(n)} = \boldsymbol{1}\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In this case, we start from <span class="math inline">\(\boldsymbol\beta^{(n)} = \boldsymbol{1}\)</span> and compute <span class="math inline">\(\boldsymbol\beta^{(m-1)}\)</span> in terms of <span class="math inline">\(\boldsymbol\beta^{(m)}\)</span> for any <span class="math inline">\(m\)</span>, i.e., we iterate <em>backwards</em>.</p>
<p><span class="math display">\[
\begin{aligned}
    \beta_k^{(m)} &amp; = \sum_{i \in \mathcal{S}} f(Z_{m+1}, \dots, Z_n \mid X_m = k, X_{m+1} = i) \Pr(X_{m+1} = i \mid X_m = k) \\
    &amp; = \sum_{i \in \mathcal{S}} f(Z_{m+1}, \dots, Z_n \mid X_{m+1} = i) P_{ki} \\
    &amp; = \sum_{i \in \mathcal{S}} f(Z_{m+2}, \dots, Z_n \mid X_{m+1} = i) f(Z_{m+1} \mid X_{m+1}) P_{ki} \\
    &amp; = \sum_{i \in \mathcal{S}} P_{ki} b_i(z_{m+1}) \beta_i^{(m+1)}
\end{aligned}
\]</span> or, in matrix notation, <span class="math display">\[
    \boldsymbol\beta^{(m)} = \boldsymbol{PB}(z_{m+1}) \boldsymbol{\beta}^{(m+1)}.
\]</span></p>
</div>
</div>
</div>
<p>We can use this formula to compute all backward probabilities iteratively, and this procedure is called the backward algorithm.</p>
<p>It turns out that the state probabilities <span class="math inline">\(\eta_k^{(m)}\)</span> can be computed easily from the forward and backward probabilities.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 5.4
</div>
</div>
<div class="callout-body-container callout-body">
<p>The state probabilities can be computed as <span class="math display">\[
\eta_k^{(m)} = \frac{\alpha_k^{(m)} \beta_k^{(m)}}{\sum_{j\in \mathcal{S}} \alpha_j^{(m)} \beta_j^{(m)}}
\]</span> where <span class="math inline">\(\alpha_k^{(m)}\)</span> and <span class="math inline">\(\beta_k^{(m)}\)</span> are the forward and backward probabilities, respectively.</p>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>From the definition of conditional probability, and using shorthand notation, <span class="math display">\[
\begin{aligned}
    \eta_k^{(m)} &amp; = \Pr(X_m = k \mid Z_0 \dots, Z_n) \\
        &amp; = \frac{f(X_m = k, Z_0, \dots, Z_n)}{f(Z_0, \dots, Z_n)}.
\end{aligned}
\]</span></p>
<p>The numerator is <span class="math display">\[
    f(X_m = k, Z_0, \dots, Z_n) = f(Z_0, \dots, Z_n \mid X_m = k) \Pr(X_m = k)
\]</span></p>
<p>We note that <span class="math inline">\((Z_0, \dots, Z_m)\)</span> and <span class="math inline">\((Z_{m+1}, \dots, Z_n)\)</span> are conditionally independent given <span class="math inline">\(X_m\)</span> (due to the structure of a hidden Markov model). So this can be rearranged to <span class="math display">\[
\begin{aligned}
    f(X_m = k, Z_0, \dots, Z_n) &amp; = f(Z_0, \dots, Z_m \mid X_m = k) f(Z_{m+1}, \dots, Z_n \mid X_m = k) \Pr(X_m = k) \\
    &amp; = f(Z_0, \dots, Z_m, X_m = k) f(Z_{m+1}, \dots, Z_n \mid X_m = k) \\
    &amp; = \alpha_k^{(m)} \beta_k^{(m)}.
\end{aligned}
\]</span></p>
<p>Applying the law of total probability on the denominator, we also have <span class="math display">\[
\begin{aligned}
f(Z_0, \dots, Z_n) &amp; = \sum_{j \in \mathcal{S}} f(X_m = j, Z_0, \dots, Z_n) \\
    &amp; = \sum_{j \in \mathcal{S}} \alpha_j^{(m)} \beta_j^{(m)}
\end{aligned}
\]</span></p>
<p>We find the required result, <span class="math display">\[
\eta_k^{(m)} = \frac{\alpha_k^{(m)} \beta_k^{(m)}}{\sum_{j\in \mathcal{S}} \alpha_j^{(m)} \beta_j^{(m)}}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="some-examples" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="some-examples"><span class="header-section-number">5.5</span> Some examples</h2>
<section id="animal-telemetry" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="animal-telemetry"><span class="header-section-number">5.5.1</span> Animal telemetry</h3>
<p>In the last couple of decades, it has become increasingly possible to track wild animals using telemetry devices. This could be a GPS collar on a polar bear, a depth sensor on a beaked whale, or an accelerometer on an albatross. The resulting data contain an incredible amount of information about the behaviour of those animals, which would be very difficult to obtain otherwise.</p>
<p>Consider the acceleration data shown in <a href="#fig-albatross-data" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-albatross-data</span></a>, which comes from an albatross tagged in South Georgia (a small island in the South Atlantic) and was analysed by <span class="citation" data-cites="conners2021">Conners et al. (<a href="#ref-conners2021" role="doc-biblioref">2021</a>)</span>. The variable shown here is a derived metric of “heave acceleration”, i.e., acceleration along the bird’s up-down axis, measured every 30 seconds. It is clear that the distribution of acceleration is multimodal, and it looks from the time series plot that there is strong autocorrelation: high acceleration is likely to be followed by high acceleration. The multimodality suggests that a mixture model might be adequate, and the autocorrelation suggests that some dependence is required, making hidden Markov models a natural choice.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-albatross-data" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-albatross-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-albatross-data-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-albatross-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: Albatross accelerometer data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, say that we use a hidden Markov model with three states, i.e., <span class="math inline">\(X_m \in \mathcal{S} = \{ 0, 1, 2 \}\)</span>, to identify three mixture components. Within each state, we choose to model the acceleration with a normal distribution, i.e., <span class="math inline">\(b_k\)</span> is the Gaussian probability density function (with parameters dependent on <span class="math inline">\(k\)</span>). Maximum likelihood estimation based on the forward algorithm can be used to estimate all transition probabilities of <span class="math inline">\((X_t)\)</span>, as well as the state-dependent parameters of the normal distribution of acceleration.</p>
<p><a href="#fig-albatross-results" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-albatross-results</span></a> shows the estimated state-dependent distribution <span class="math inline">\(b_k\)</span>, and the most likely sequence of unobserved states. In this model, the three states (<span class="math inline">\(X_m = 0, 1, 2\)</span>) correspond to very low, low, and high heave acceleration, respectively. A biologist could propose a tentative interpretation in terms of albatross behaviour, and the model could then be used to distinguish phases where the animal is resting on water and flying, for example.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-albatross-results" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-albatross-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-albatross-results-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-albatross-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.6: Results of albatross analysis.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Another output of the model is the estimated transition probability matrix of the state process, <span class="math display">\[
    \widehat{\boldsymbol{P}} =
    \begin{pmatrix}
        0.783 &amp; 0.21 &amp; 0.007 \\
        0.042 &amp; 0.935 &amp; 0.023 \\
        0 &amp; 0.013 &amp; 0.987
    \end{pmatrix}
\]</span> The large diagonal transition probabilities reflect a strong tendency to persist in each state. We can use the results from Chapter 2 to get insights into the behaviour of albatross. For example, from <span class="math inline">\(\widehat{\boldsymbol{P}}\)</span>, we can see that the expected holding times in the three states are <span class="math display">\[
    \frac{1}{1 - 0.783} = 4.6,\quad
    \frac{1}{1 - 0.935} = 15,\quad \text{and }
    \frac{1}{1 - 0.987} = 77.
\]</span></p>
<p>We can compute the stationary distribution of the Markov chain using any of the methods from Chapter 2, which gives us an estimate of the long-run proportion of time that the bird spends in each behavioural state. Here, we find <span class="math display">\[
    \boldsymbol{\pi} = (0.06, 0.33, 0.61).
\]</span></p>
<p>These results all suggest that the albatross spends most of its time in the state <span class="math inline">\(X = 2\)</span>.</p>
</section>
<section id="oil-price" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="oil-price"><span class="header-section-number">5.5.2</span> Oil price</h3>
<p>We now turn to the problem of understanding the dynamics of oil prices through time. <a href="#fig-oil-data" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-oil-data</span></a> shows the daily changes in oil prices in the USA between 1986 and 2023, obtained from the <a href="https://www.eia.gov/">US Energy Information Administration</a>. The histogram does not display multimodality this time, but the distribution looks heavy-tailed and would not be modelled well with something like a normal distribution. This is another reason to use a mixture model. The time series plot shows an interesting pattern of alternance between long periods of high and low variability. We might interpret high variability as a sign of financial instability and, indeed, there are large price changes after the 2008 financial crisis, as well as during the first few months of the Covid-19 pandemic.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-oil-data" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-oil-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-oil-data-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-oil-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.7: Oil price data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Just like in the previous example, we analyse the data with a hidden Markov model with three states and normal state-dependent distributions. Using maximum likelihood estimation, we get estimates of the transition probabilities and of the parameters of the normal distribution in each state.</p>
<p><a href="#fig-oil-results" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-oil-results</span></a> shows the estimated state-dependent distributions, and the most likely state sequence for the fitted hidden Markov model. In contrast with the albatross example, what distinguishes the three states this time is not the mean of the distributions, but their variances. They roughly represent low (<span class="math inline">\(X_m = 0\)</span>), intermediate (<span class="math inline">\(X_m = 1\)</span>), and high (<span class="math inline">\(X_m = 2\)</span>) variances, corresponding to different levels of financial instability.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-oil-results" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-oil-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="05_HMM_files/figure-html/fig-oil-results-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-oil-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.8: Results of oil price analysis.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The estimated transition probability matrix is <span class="math display">\[
    \widehat{\boldsymbol{P}} =
    \begin{pmatrix}
        0.989 &amp; 0.011 &amp; 0 \\
        0.007 &amp; 0.98 &amp; 0.012 \\
        0 &amp; 0.1 &amp; 0.9
    \end{pmatrix}
\]</span> indicating that there is strong autocorrelation in the state process. The expected holding times (measured in days) are <span class="math display">\[
    \frac{1}{1 - 0.989} = 91,\quad
    \frac{1}{1 - 0.98} = 50,\quad \text{and }
    \frac{1}{1 - 0.9} = 10,
\]</span> and the stationary distribution is <span class="math display">\[
    \boldsymbol{\pi} = (0.38, 0.56, 0.07).
\]</span></p>
</section>
</section>
<section id="problems" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="problems"><span class="header-section-number">5.6</span> Problems</h2>
<ol type="1">
<li><p>You have been recording your daily average heart rate. You see that it usually fluctuates around some baseline level, but that it is noticeably affected by two factors: your heart rate tends to be higher than usual in the days preceding an exam, and your heart rate tends to be lower than usual during holidays.</p>
<p>Propose a hidden Markov model formulation for this situation. You should specify the number of states and their interpretation, define the observation variable, and choose a relevant observation distribution.</p></li>
<li><p>We consider a hidden Markov model with state process <span class="math inline">\((X_n)\)</span> and observation process <span class="math inline">\((Z_n)\)</span>, where <span class="math inline">\(X_n \in \{ 1, 2 \}\)</span> and <span class="math display">\[
\begin{aligned}
&amp; \boldsymbol{u}^{(0)} = \left(\frac{1}{2}, \frac{1}{2}\right),\\[2mm]
&amp; \boldsymbol{P} =
\begin{pmatrix}
     \frac{2}{3} &amp; \frac{1}{3} \\[2mm]
     \frac{1}{4} &amp; \frac{3}{4}
\end{pmatrix},\\
&amp; Z_n \mid X_n \sim
     \begin{cases}
     \text{Binomial}(2, \frac{1}{4}) &amp; \text{if } X_n = 1, \\[2mm]
     \text{Binomial}(2, \frac{3}{4}) &amp; \text{if } X_n = 2.
     \end{cases}
\end{aligned}
\]</span></p>
<ol type="a">
<li><p>Derive the marginal distribution of <span class="math inline">\(Z_1\)</span>.</p></li>
<li><p>Derive the expected value of <span class="math inline">\(Z_1\)</span>.</p></li>
</ol></li>
<li><p>We consider a hidden Markov model with state process <span class="math inline">\((X_n)\)</span> and observation process <span class="math inline">\((Z_n)\)</span>, where <span class="math inline">\(X_n \in \{ 1, 2 \}\)</span> and <span class="math display">\[
\begin{aligned}
&amp; \boldsymbol{u}^{(0)} = \left(\frac{1}{2}, \frac{1}{2}\right),\\[2mm]
&amp; \boldsymbol{P} =
\begin{pmatrix}
     \frac{1}{2} &amp; \frac{1}{2} \\[2mm]
     \frac{3}{10} &amp; \frac{7}{10}
\end{pmatrix},\\
&amp; Z_n \mid X_n \sim
\begin{cases}
     \text{Bernoulli}(\frac{1}{2}) &amp; \text{if } X_n = 1, \\[2mm]
     \text{Bernoulli}(\frac{4}{5}) &amp; \text{if } X_n = 2.
\end{cases}
\end{aligned}
\]</span></p>
<p>The table at the top of the next page lists the eight possible state sequences over <span class="math inline">\(n \in \{ 0, 1, 2 \}\)</span>, and summarises a few relevant quantities. The “product” column gives the product of the terms in columns 4 through 9.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 6%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 8%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(x_0\)</span></th>
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(b_{x_0}(1)\)</span></th>
<th><span class="math inline">\(b_{x_1}(1)\)</span></th>
<th><span class="math inline">\(b_{x_2}(1)\)</span></th>
<th><span class="math inline">\(u^{(0)}_{x_{0}}\)</span></th>
<th><span class="math inline">\(P_{x_0, x_1}\)</span></th>
<th><span class="math inline">\(P_{x_1, x_2}\)</span></th>
<th>product</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{64}\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>2</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{1}{2}}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{1}{40}}\)</span></td>
</tr>
<tr class="odd">
<td>1</td>
<td>2</td>
<td>1</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{1}{2}}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{3}{10}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{3}{200}}\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td>2</td>
<td>2</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{7}{10}\)</span></td>
<td><span class="math inline">\(\frac{7}{125}\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>1</td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{3}{10}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{1}{2}}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{3}{200}}\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
<td>2</td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{3}{10}}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\textcolor{white}{\frac{3}{125}}\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td>2</td>
<td>1</td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{7}{10}\)</span></td>
<td><span class="math inline">\(\frac{3}{10}\)</span></td>
<td><span class="math inline">\(\frac{21}{625}\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td>2</td>
<td>2</td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{4}{5}\)</span></td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
<td><span class="math inline">\(\frac{7}{10}\)</span></td>
<td><span class="math inline">\(\frac{7}{10}\)</span></td>
<td><span class="math inline">\(\frac{392}{3125}\)</span></td>
</tr>
</tbody>
</table>
<ol type="a">
<li><p>Fill the missing numbers in the table.</p></li>
<li><p>Given the model parameters and the observation sequence <span class="math inline">\(Z_0 = Z_1 = Z_2 = 1\)</span>, what is the most likely state sequence? In one or two sentences, explain intuitively why this is the case.</p></li>
<li><p>Derive <span class="math inline">\(\Pr(Z_0 = 1, Z_1 = 1, Z_2 = 1)\)</span> using the table.</p></li>
<li><p>Derive <span class="math inline">\(\Pr(Z_1 = 1)\)</span>, <span class="math inline">\(\Pr(Z_0 = 1, Z_1 = 1)\)</span>, and <span class="math inline">\(\Pr(Z_1 = 1, Z_2 = 1)\)</span>. You can use the fact that, for any <span class="math inline">\(n\)</span>, <span class="math inline">\(\Pr(Z_n = z_n, Z_{n+1} = z_{n+1}) = \boldsymbol{u}^{(n)} \boldsymbol{B}(z_n) \boldsymbol{P} \boldsymbol{B}(z_{n+1}) \boldsymbol{1}^\intercal\)</span>.</p></li>
<li><p>Derive <span class="math inline">\(\Pr(Z_2 = 1 \mid Z_0 = 1, Z_1 = 1)\)</span>.</p></li>
<li><p>Derive <span class="math inline">\(\Pr(Z_2 = 1 \mid Z_1 = 1)\)</span>.</p></li>
<li><p>Conclude about whether the observation process <span class="math inline">\((Z_n)\)</span> of a hidden Markov model is generally a Markov process.</p></li>
</ol></li>
<li><p>Consider the 3-state hidden Markov model (<span class="math inline">\(X_n \in \{ 1, 2, 3 \}\)</span>) with initial state <span class="math inline">\(X_0 = 1\)</span> and transition probability matrix <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
     0.9 &amp; 0.1 &amp; 0 \\
     0.2 &amp; 0.7 &amp; 0.1 \\
     0.3 &amp; 0 &amp; 0.7
\end{pmatrix}.
\]</span></p>
<p>This model has two observation processes <span class="math inline">\((Y_n)\)</span> and <span class="math inline">\((Z_n)\)</span> defined by <span class="math display">\[
Y_n \mid X_n \sim
\begin{cases}
     \text{Exp}(0.5) &amp; \text{if } X_n = 1 \\
     \text{Exp}(2) &amp; \text{if } X_n = 2 \\
     \text{Exp}(5) &amp; \text{if } X_n = 3
\end{cases}\quad\text{and}\quad
Z_n \mid X_n \sim
\begin{cases}
     N(0, 5^2) &amp; \text{if } X_n = 1 \\
     N(0, 2^2) &amp; \text{if } X_n = 2 \\
     N(0, 0.5^2) &amp; \text{if } X_n = 3
\end{cases}
\]</span></p>
<p>Moreover, <span class="math inline">\(Y_n\)</span> and <span class="math inline">\(Z_n\)</span> are independent conditionally on <span class="math inline">\(X_n\)</span>. Using simulations in R, compute a numerical approximation of <span class="math inline">\(\Pr(Z_{100} &gt; Y_{100})\)</span> (to two decimal places).</p></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-conners2021" class="csl-entry" role="listitem">
Conners, Melinda G, Théo Michelot, Eleanor I Heywood, Rachael A Orben, Richard A Phillips, Alexei L Vyssotski, Scott A Shaffer, and Lesley H Thorne. 2021. <span>“Hidden <span>Markov</span> Models Identify Major Movement Modes in Accelerometer and Magnetometer Data from Four Albatross Species.”</span> <em>Movement Ecology</em> 9 (1): 1–16.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04_markov_continuous.html" class="pagination-link" aria-label="Continuous-time Markov processes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Continuous-time Markov processes</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>