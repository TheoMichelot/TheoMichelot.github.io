<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Discrete-time Markov processes – Stochastic Processes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03_poisson.html" rel="next">
<link href="./01_background.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02_markov_discrete.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Discrete-time Markov processes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./cover_spiral.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Stochastic Processes</a> 
        <div class="sidebar-tools-main">
    <a href="./Stochastic-Processes.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_markov_discrete.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Discrete-time Markov processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Poisson processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_markov_continuous.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Continuous-time Markov processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_HMM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Hidden Markov models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">2.1.1</span> Definition</a></li>
  <li><a href="#holding-times" id="toc-holding-times" class="nav-link" data-scroll-target="#holding-times"><span class="header-section-number">2.1.2</span> Holding times</a></li>
  <li><a href="#higher-order-dependence" id="toc-higher-order-dependence" class="nav-link" data-scroll-target="#higher-order-dependence"><span class="header-section-number">2.1.3</span> Higher-order dependence</a></li>
  <li><a href="#simulating-from-a-markov-process" id="toc-simulating-from-a-markov-process" class="nav-link" data-scroll-target="#simulating-from-a-markov-process"><span class="header-section-number">2.1.4</span> Simulating from a Markov process</a></li>
  </ul></li>
  <li><a href="#looking-into-the-future" id="toc-looking-into-the-future" class="nav-link" data-scroll-target="#looking-into-the-future"><span class="header-section-number">2.2</span> Looking into the future</a>
  <ul class="collapse">
  <li><a href="#chapman-kolmogorov-equations" id="toc-chapman-kolmogorov-equations" class="nav-link" data-scroll-target="#chapman-kolmogorov-equations"><span class="header-section-number">2.2.1</span> Chapman-Kolmogorov Equations</a></li>
  <li><a href="#marginal-state-distribution" id="toc-marginal-state-distribution" class="nav-link" data-scroll-target="#marginal-state-distribution"><span class="header-section-number">2.2.2</span> Marginal state distribution</a></li>
  </ul></li>
  <li><a href="#first-step-analysis" id="toc-first-step-analysis" class="nav-link" data-scroll-target="#first-step-analysis"><span class="header-section-number">2.3</span> First step analysis</a></li>
  <li><a href="#interstate-travel" id="toc-interstate-travel" class="nav-link" data-scroll-target="#interstate-travel"><span class="header-section-number">2.4</span> Interstate travel</a>
  <ul class="collapse">
  <li><a href="#communication-and-reducibility" id="toc-communication-and-reducibility" class="nav-link" data-scroll-target="#communication-and-reducibility"><span class="header-section-number">2.4.1</span> Communication and reducibility</a></li>
  <li><a href="#transience-and-recurrence" id="toc-transience-and-recurrence" class="nav-link" data-scroll-target="#transience-and-recurrence"><span class="header-section-number">2.4.2</span> Transience and recurrence</a></li>
  <li><a href="#different-types-of-recurrence" id="toc-different-types-of-recurrence" class="nav-link" data-scroll-target="#different-types-of-recurrence"><span class="header-section-number">2.4.3</span> Different types of recurrence</a></li>
  <li><a href="#periodicity" id="toc-periodicity" class="nav-link" data-scroll-target="#periodicity"><span class="header-section-number">2.4.4</span> Periodicity</a></li>
  </ul></li>
  <li><a href="#long-run-properties" id="toc-long-run-properties" class="nav-link" data-scroll-target="#long-run-properties"><span class="header-section-number">2.5</span> Long-run properties</a>
  <ul class="collapse">
  <li><a href="#stationary-distribution" id="toc-stationary-distribution" class="nav-link" data-scroll-target="#stationary-distribution"><span class="header-section-number">2.5.1</span> Stationary distribution</a></li>
  <li><a href="#limiting-probabilities" id="toc-limiting-probabilities" class="nav-link" data-scroll-target="#limiting-probabilities"><span class="header-section-number">2.5.2</span> Limiting probabilities</a></li>
  <li><a href="#long-run-proportions" id="toc-long-run-proportions" class="nav-link" data-scroll-target="#long-run-proportions"><span class="header-section-number">2.5.3</span> Long-run proportions</a></li>
  <li><a href="#calculating-the-stationary-distribution" id="toc-calculating-the-stationary-distribution" class="nav-link" data-scroll-target="#calculating-the-stationary-distribution"><span class="header-section-number">2.5.4</span> Calculating the stationary distribution</a></li>
  </ul></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference"><span class="header-section-number">2.6</span> Statistical inference</a>
  <ul class="collapse">
  <li><a href="#likelihood-function" id="toc-likelihood-function" class="nav-link" data-scroll-target="#likelihood-function"><span class="header-section-number">2.6.1</span> Likelihood function</a></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation"><span class="header-section-number">2.6.2</span> Parameter estimation</a></li>
  </ul></li>
  <li><a href="#markov-chains-with-uncountable-state-space" id="toc-markov-chains-with-uncountable-state-space" class="nav-link" data-scroll-target="#markov-chains-with-uncountable-state-space"><span class="header-section-number">2.7</span> Markov chains with uncountable state space</a></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications"><span class="header-section-number">2.8</span> Applications</a>
  <ul class="collapse">
  <li><a href="#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo" class="nav-link" data-scroll-target="#markov-chain-monte-carlo"><span class="header-section-number">2.8.1</span> Markov chain Monte Carlo</a></li>
  <li><a href="#google-search" id="toc-google-search" class="nav-link" data-scroll-target="#google-search"><span class="header-section-number">2.8.2</span> Google search</a></li>
  <li><a href="#n-gram-models-predictive-text" id="toc-n-gram-models-predictive-text" class="nav-link" data-scroll-target="#n-gram-models-predictive-text"><span class="header-section-number">2.8.3</span> <span class="math inline">\(N\)</span>-gram models (predictive text)</a></li>
  </ul></li>
  <li><a href="#problems" id="toc-problems" class="nav-link" data-scroll-target="#problems"><span class="header-section-number">2.9</span> Problems</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Discrete-time Markov processes</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- -   I need to identify which results work in the finite case, the countable case, and the uncountable case.

-   It might be good to show that a follow-up from the Markov property is that $\Pr(X_{t+1} \mid X_n, X_m) = \Pr(X_{t+1} \mid X_n)$ for any $t+1 \leq n \leq m$, not just for $n = t$. We use this implicitly in the proof of the Chapman-Kolmogorov equations, and it's also closer to the intuition of the Markov property in continuous time. -->
<p>We first consider the case of a discrete-time stochastic process <span class="math inline">\((X_n)_{n \in \mathbb{N}}\)</span>, defined over a countable state space <span class="math inline">\(\mathcal{S}\)</span> (i.e., <span class="math inline">\(X_n \in \mathcal{S}\)</span>). In this chapter, we denote the time index <span class="math inline">\(n\)</span> rather than <span class="math inline">\(t\)</span> as a reminder that time is discrete. We will later cover the case of an uncountable state space (Section 2.6), and continuous-time processes (Chapter 4).</p>
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>A key feature of a stochastic process is its dependence structure, i.e., how successive values of the process depend on each other. The simplest assumption would be that the <span class="math inline">\(X_n\)</span> are independent random variables, but this is an unrealistic premise in many situations. The next simplest assumption would be that <span class="math inline">\(X_{n+1}\)</span> is dependent on <span class="math inline">\(X_n\)</span>, but not on previous values of the process (at least, not conditional on <span class="math inline">\(X_n\)</span>). This is called the Markov assumption, and we will see that it captures the dependence of many real-world processes, despite its apparent simplicity.</p>
<section id="definition" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">2.1.1</span> Definition</h3>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.1
</div>
</div>
<div class="callout-body-container callout-body">
<p>A discrete-time stochastic process <span class="math inline">\((X_n)\)</span> is called a <strong>Markov process</strong> if it satisfies <span class="math display">\[
\Pr(X_{n+1} = j \mid X_{n}, X_{n-1}, \dots, X_{0}) = \Pr(X_{n+1} = j \mid X_{n}),
\]</span> for all <span class="math inline">\(j \in \mathcal{S}\)</span>. This is called the Markov property or Markov assumption.</p>
</div>
</div>
<p>There are several ways we could describe this property in words:</p>
<ul>
<li><span class="math inline">\(X_n\)</span> contains all the information we need about the history of the process to determine the distribution of <span class="math inline">\(X_{n+1}\)</span>;</li>
<li>the future is independent of the past, conditionally on the present;</li>
<li>the process has no memory (the Markov property is sometimes called “memorylessness”).</li>
</ul>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.1
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Perhaps the most common textbook example of a Markov chain is a simplified weather model. Let’s assume that the weather on a given day is either sunny or cloudy, and that tomorrow’s weather depends on today’s weather, but that is is independent of previous days (conditionally on today). This system can be modelled with a Markov chain with state space <span class="math inline">\(\mathcal{S} = \{ \text{sunny}, \text{cloudy}\}\)</span>.<br>
</p></li>
<li><p>Consider the following game. You repeatedly throw a die; if it falls on 6, you win $10 and, if it falls on any other number, you lose $1. Let <span class="math inline">\(X_n\)</span> be the amount you have won (or lost) after <span class="math inline">\(n\)</span> rounds, starting from <span class="math inline">\(X_0 = 0\)</span>. The process <span class="math inline">\((X_n)\)</span> satisfies the Markov property because, if you know <span class="math inline">\(X_n\)</span>, knowing <span class="math inline">\(X_{n-1}, X_{n-2}, \dots\)</span> does not give you any extra information to predict what <span class="math inline">\(X_{n+1}\)</span> will be. (Try to write the conditional distribution of <span class="math inline">\(X_{n+1}\)</span> given <span class="math inline">\(X_n\)</span> in this example.)</p></li>
</ol>
</div>
</div>
</div>
<p>By abuse of language, the Markov property is sometimes described by saying that <span class="math inline">\(X_{n+1}\)</span> only depends on <span class="math inline">\(X_n\)</span> and not on previous values of the process (<span class="math inline">\(X_{n-1}, X_{n-2}, \dots\)</span>). Note that, in this version, there is no explicit mention of the conditional nature of this statement. If we do not condition on <span class="math inline">\(X_t\)</span>, then <span class="math inline">\(X_{n+1}\)</span> is in fact dependent on <span class="math inline">\(X_{n-1}\)</span>, as will become clearer later.</p>
<p>We often use the word “<strong>state</strong>” when describing Markov processes, and it can refer to two things:</p>
<ul>
<li>the state of the process at time <span class="math inline">\(n\)</span> is its value <span class="math inline">\(X_n\)</span>;</li>
<li>the states of the process are elements of its state space (e.g., “sunny” and “cloudy” in the weather example).</li>
</ul>
<p>We can then rephrase the Markov property as: the state of the process at time <span class="math inline">\(n+1\)</span> is independent of its state at time <span class="math inline">\(n-1\)</span> (and before), conditional on its state at time <span class="math inline">\(n\)</span>.</p>
<p>In the definition of a Markov process that we gave above, the process is assumed to be defined over a countable set <span class="math inline">\(\mathcal{S}\)</span>. An important special case is when <span class="math inline">\(\mathcal{S}\)</span> is finite, i.e., the process can take on a finite number of values. In such a case, we sometimes refer to a <span class="math inline">\(N\)</span>-state Markov process for a process defined over a set of size <span class="math inline">\(N\)</span>. For Markov processes over countable (finite or infinite) sets, it is often convenient to denote the state space using the integers, e.g., <span class="math inline">\(\mathcal{S} = \{ 0, 1, 2 \}\)</span> or <span class="math inline">\(\mathcal{S} = \mathbb{Z}\)</span>. Despite this notation, it is important to remember that, in many applications, the states are qualitative rather than quantitative, and the integers are merely indices to distinguish them. We can also sometimes label the states using letters or some other symbols to make this more explicit.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.2: random walks
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Random walks are a broad class of stochastic processes, which are widely used in science. Here we consider two examples, to illustrate Markov processes with finite or infinite state spaces.<br>
</p>
<ol type="1">
<li><p><strong>Finite state space:</strong> Consider the position of a player’s token on a Monopoly board. The board has 40 squares in total and, at each round, the player moves between 2 and 12 squares forward based on the throw of two dice (ignoring other special rules). The board is a loop, so the 40th square is next to the 1st. If we let <span class="math inline">\(X_n \in \{ 0, 1, \dots, 39 \}\)</span> be the position of the token after <span class="math inline">\(n\)</span> rounds, then <span class="math inline">\((X_n)\)</span> is a Markov process.<br>
</p></li>
<li><p><strong>Countably infinite state space:</strong> Let <span class="math inline">\(X_n\)</span> be the number of pandas in the world on day <span class="math inline">\(n\)</span>, and assume that, on a given day, the population can increase by 1 with probability <span class="math inline">\(p\)</span>, decrease by 1 with probability <span class="math inline">\(q\)</span>, or stay the same with probability <span class="math inline">\(1 - p - q\)</span>. If the population decreases to 0, then it cannot change anymore. This defines a Markov process over the non-negative integers.</p></li>
</ol>
<!---
 3. **Uncountably infinite state space:** Let $Z_n$ be the amount of rain on day $n$, and $X_n = \sum_{i=0}^n Z_i$ the total amount of rainfall since day 0 (which could for example be January 1). $(X_n)$ is a Markov process, and its state space is the non-negative real numbers.  
--->
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Terminology: Markov process and Markov chain
</div>
</div>
<div class="callout-body-container callout-body">
<p>The term “Markov chain” is used very commonly, but its meaning is somewhat inconsistent. Depending on the source, it can refer to:</p>
<ol type="1">
<li><p>any Markov process;</p></li>
<li><p>a Markov process with discrete state space;</p></li>
<li><p>a Markov process in discrete time.<br>
</p></li>
</ol>
<p>In these notes, we follow the first interpretation, and use Markov chain interchangeably with Markov process.</p>
</div>
</div>
<p>Due to the Markov property, the dynamics of a Markov process with countable state space can be specified in terms of the probabilities of moving between any two states over one time interval, given by <span class="math inline">\(\Pr(X_{n + 1} = j \mid X_{n} = i)\)</span> for any <span class="math inline">\(i, j \in \mathcal{S}\)</span>. If the state space is of finite size <span class="math inline">\(\vert \mathcal{S}\vert = N\)</span>, there are <span class="math inline">\(N^2\)</span> such probabilities.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.2
</div>
</div>
<div class="callout-body-container callout-body">
<p>The one-step <strong>transition probability</strong> from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> is <span class="math display">\[
P_{ij} = \Pr(X_{n+1} = j \mid X_n = i)
\]</span> for <span class="math inline">\(i, j \in \mathcal{S}\)</span>.<br>
</p>
<p>The one-step <strong>transition probability matrix</strong> is <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
P_{00} &amp; P_{01} &amp; P_{02} &amp; \cdots \\
P_{10} &amp; P_{11} &amp; P_{12} &amp; \cdots \\
P_{20} &amp; P_{21} &amp; P_{22} &amp; \cdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{pmatrix}
\]</span> We also call it the transition matrix.</p>
</div>
</div>
<p><strong>Remark:</strong> The transition probabilities are subject to the following constraints.</p>
<ul>
<li><span class="math inline">\(P_{ij} \in [0, 1]\)</span>, for any <span class="math inline">\(i, j \in \mathcal{S}\)</span></li>
<li><span class="math inline">\(\displaystyle \sum_{j \in \mathcal{S}} P_{ij} = 1\)</span>, for any <span class="math inline">\(i \in \mathcal{S}\)</span></li>
</ul>
<p>The first point follows from the definition of probabilities, and the second point reflects the necessity that <span class="math inline">\(X_{n+1} \in \mathcal{S}\)</span> (i.e., there must be a <span class="math inline">\(j \in \mathcal{S}\)</span> for which <span class="math inline">\(X_{n+1} = j\)</span>). A square matrix whose entries satisfy those two conditions is called a stochastic matrix (or sometimes a “right” stochastic matrix). Each row of a stochastic matrix is a (discrete) probability distribution.</p>
<p>If the state space of the Markov chain is finite, i.e., <span class="math inline">\(\vert \mathcal{S} \vert = N &lt; \infty\)</span>, then the transition probability matrix is an <span class="math inline">\(N \times N\)</span> matrix. In the case of an countable infinite state space, however, the matrix is infinite (i.e., it has an infinite number of rows and columns). Although this seems to complicate things, most matrix operations are still well defined for infinite matrices, and the results described below hold for any countable state space. In practice, the main challenge associated with the infinite state space is that we cannot write out the full matrix, either by hand or to store it in a computer.</p>
<p>In this chapter, we will assume that the transition probabilities do not depend on the time step <span class="math inline">\(n\)</span>, i.e., they are constant through time.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.3
</div>
</div>
<div class="callout-body-container callout-body">
<p>A Markov chain is <strong>time-homogeneous</strong> if, for any <span class="math inline">\(n = 0, 1, \dots\)</span>, <span class="math display">\[
\Pr(X_{n + 1} = j \mid X_{n} = i) = \Pr(X_{1} = j \mid X_{0} = i)
\]</span> where <span class="math inline">\(i, j \in \mathcal{S}\)</span>.<br>
</p>
<p>That is, the probability of going from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> does not depend on <span class="math inline">\(n\)</span>.</p>
</div>
</div>
<p>It is common to represent a Markov chain as a <strong>transition graph</strong>, with one node for each state, and arrows showing all possible transitions. This can be viewed as a weighted graph, where the weight of each edge is the corresponding transition probability.</p>
<p><strong>Example:</strong> Consider the 3-state Markov chain with transition probability matrix <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
0.5 &amp; 0.5 &amp; 0 \\
0 &amp; 0.3 &amp; 0.7 \\
0.1 &amp; 0 &amp; 0.9
\end{pmatrix}
\]</span></p>
<p>If we label the three states as “a”, “b”, and “c”, we can represent the transition structure of the process as shown in <a href="#fig-markov-graph" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-graph</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-markov-graph" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-markov-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-markov-graph">graph LR;
a((a)) --&gt;|0.5| b((b))
a --&gt;|0.5| a
b --&gt;|0.7| c((c))
b --&gt;|0.3| b
c --&gt;|0.9| c
c --&gt;|0.1| a
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-markov-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Example transition graph of 3-state Markov chain
</figcaption>
</figure>
</div>
</div>
</div>
<p>A realisation from a Markov chain is a sequence of states for some set of time indices. For example, (a, a, b, c, c, c, c, c, c, a) is one possible realisation of the 3-state Markov chain shown in <a href="#fig-markov-graph" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-graph</span></a> over 10 time steps. We can display those as time series graphs, with time along the <span class="math inline">\(x\)</span> axis and state along the <span class="math inline">\(y\)</span> axis, as long as we remember that the ordering of the states is often arbitrary. Four example realisations from the Markov chain of <a href="#fig-markov-graph" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-graph</span></a> are shown in <a href="#fig-markov-sim" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-sim</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-markov-sim" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-markov-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-markov-sim-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-markov-sim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Four realisations of a 3-state Markov process, where the initial state was chosen at random.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-markov-sim" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-sim</span></a> makes it clear that, if we don’t condition on <span class="math inline">\(X_n\)</span>, then <span class="math inline">\(X_{n+1}\)</span> is dependent on previous states. For example, if all we know is that <span class="math inline">\(X_{n-1} = \text{c}\)</span> (and we don’t know <span class="math inline">\(X_n\)</span>), this still gives us quite a bit of information about <span class="math inline">\(X_{n+1}\)</span>. In this example, we know that <span class="math inline">\(X_{n+1}\)</span> is most likely to also be c, because the process tends to stay in that state for many consecutive time steps.</p>
</section>
<section id="holding-times" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="holding-times"><span class="header-section-number">2.1.2</span> Holding times</h3>
<p>One way to understand the Markov assumption, and whether it is violated in a given context, is to think about how long the process spends in a given state (before switching to another state). Let <span class="math inline">\(D_i\)</span> denote the number of consecutive time steps spent in state <span class="math inline">\(i\)</span>, called the holding time (or dwell time). The event <span class="math inline">\(D_i = 1\)</span> corresponds to the situation where the process switches out of state <span class="math inline">\(i\)</span> in the first time step, which has probability <span class="math inline">\(1 - P_{ii}\)</span> (i.e., one minus the probability of remaining in state <span class="math inline">\(i\)</span>). The event <span class="math inline">\(D_i = 2\)</span> requires remaining in state <span class="math inline">\(i\)</span> in the first time step (with probability <span class="math inline">\(P_{ii}\)</span>) and switching out of state <span class="math inline">\(i\)</span> in the second time step (with probability <span class="math inline">\(1 - P_{ii}\)</span>), so it has probability <span class="math inline">\(P_{ii} (1 - P_{ii})\)</span>. We can repeat this reasoning to find the general formula: <span class="math display">\[
\Pr(D_i = k) = P_{ii}^{k-1} (1 - P_{ii}),
\]</span> because <span class="math inline">\(D_i = k\)</span> means that the process remained in state <span class="math inline">\(i\)</span> for <span class="math inline">\(k - 1\)</span> time steps, and then switched to another state.</p>
<p>This is the probability mass function of the geometric distribution with parameter <span class="math inline">\(1 - P_{ii}\)</span>. <a href="#fig-geom-dist" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-geom-dist</span></a> shows the graph of this function for different values of the probability parameter <span class="math inline">\(p = 1 - P_{ii}\)</span>. Although the decay rate of the distribution depends on the transition probability, its mode is always 1, i.e., the most likely holding time is 1 regardless of the transition probabilities.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-geom-dist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-geom-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-geom-dist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-geom-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Probability mass function of the geometric distribution, for different values of the probability parameter.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The mean of the geometric distribution is the inverse of the probability parameter so, in the context of the Markov chain, <span class="math display">\[
E[D_i] = \frac{1}{1 - \gamma_{ii}}
\]</span></p>
<p>For example, the expected holding times for the Markov chain shown in <a href="#fig-markov-graph" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-graph</span></a> are <span class="math inline">\(E[D_1] = 1/(1 - 0.5) = 2\)</span>, <span class="math inline">\(E[D_2] = 1/(1 - 0.3) = 1.43\)</span>, and <span class="math inline">\(E[D_3] = 1/(1 - 0.9) = 10\)</span>. This is consistent with the patterns observed in the simulated realisations of <a href="#fig-markov-sim" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-sim</span></a>, where the process tends to spend much longer in state 3 than in states 1 and 2.</p>
</section>
<section id="higher-order-dependence" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="higher-order-dependence"><span class="header-section-number">2.1.3</span> Higher-order dependence</h3>
<p>The Markov property might seem like a strong assumption in many situations. After all, most real-world processes have very complex dependence structures. For example, tomorrow’s weather likely depends on more than just today’s weather. But it is important to remember that stochastic models, like any models, are only supposed to be an approximation. The Markov property turns out to be a pretty good approximation to many complex phenomena.</p>
<p>There are several ways to relax the Markov assumption while preserving some of the convenient mathematical properties of Markov chains.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.4
</div>
</div>
<div class="callout-body-container callout-body">
<p>A discrete-time stochastic process <span class="math inline">\((X_n)\)</span> is called a <strong><span class="math inline">\(p\)</span>-th order Markov process</strong> if it satisfies <span class="math display">\[
\Pr(X_{n} = j \mid X_{n-1}, X_{n-2}, \dots, X_{0}) =
\Pr(X_{n} = j \mid X_{n-1}, \dots, X_{n-p}),
\]</span> for all <span class="math inline">\(j \in \mathcal{S}\)</span>.</p>
</div>
</div>
<p>Higher-order Markov processes might seem considerably more flexible than (first-order) Markov processes, but they are also harder to implement. Fortunately, they can be written as first-order Markov processes with an expanded state space, such that all results in this chapter can be applied to them directly. To convert a <span class="math inline">\(p\)</span>-th order Markov chain into a first-order Markov chain, we can define the new state space to be the set of all possible sequences of <span class="math inline">\(p\)</span> states (let’s call these new states “expanded states”). So, we will focus on first-order Markov processes, keeping in mind that they can be used very generally.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.3
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For example, if we have a second-order Markov chain with states {A, B, C}, the expanded state space would be {AA, AB, AC, BA, BB, BC, CA, CB, CC}. Once we have defined the expanded state space, we can define a first-order Markov chain where each state is one of the expanded states (i.e., a sequence of <span class="math inline">\(p\)</span> states). By expanding the state space in this way, we can convert a higher-order Markov chain into a first-order Markov chain, which is easier to study. Note that not all transitions are possible in this new first-order Markov chain; for example, the process cannot transition from AA to BA.<br>
</p>
<p>If we label the states from 1 to 9 in the order shown above, the transition probability matrix for this example would be <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
P_{11} &amp; P_{12} &amp; P_{13} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; P_{24} &amp; P_{25} &amp; P_{26} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; P_{37} &amp; P_{38} &amp; P_{39} \\
P_{41} &amp; P_{42} &amp; P_{43} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; P_{54} &amp; P_{55} &amp; P_{56} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; P_{67} &amp; P_{68} &amp; P_{69} \\
P_{71} &amp; P_{72} &amp; P_{73} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; P_{84} &amp; P_{85} &amp; P_{86} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; P_{97} &amp; P_{98} &amp; P_{99} \\
\end{pmatrix},
\]</span> where <span class="math inline">\(P_{11} = \Pr(X_n = \text{AA} \mid X_{tn1} = \text{AA})\)</span>, <span class="math inline">\(P_{12} = \Pr(X_n = \text{AB} \mid X_{n-1} = \text{AA})\)</span>, and so on.<br>
</p>
<p>This approach can in principle be used to represent any high-order Markov process, but the state space grows rapidly with the number of states and the order.</p>
</div>
</div>
</div>
<!-- ::: {.callout-note icon="false"}
## Proposition 2.1

A $p$-th order Markov process with $N$ states has $N^{p+1}$ non-zero transition probabilities.
:::

::: {.callout-important icon="false" collapse="true"}
## Proof

If there are $N$ states, then there are $N^p$ sequences of states of length $p$; this is the size of the expanded state space. There are $N$ possible transitions out of each expanded state: one to each of the states. So, there are $N^p \times N = N^{p+1}$ non-zero transition probabilities.
::: -->
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
<section id="simulating-from-a-markov-process" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="simulating-from-a-markov-process"><span class="header-section-number">2.1.4</span> Simulating from a Markov process</h3>
<p>Given an initial distribution and a transition probability matrix, we can simulate from a Markov chain by iteratively sampling from a categorical distribution, e.g., using the <code>sample()</code> function in R. The code chunk below shows an example simulation over 100 time steps, for a 3-state Markov chain with state space <span class="math inline">\(\mathcal{S} = \{ 0, 1, 2 \}\)</span>, initial distribution <span class="math display">\[
    \boldsymbol{u}^{(0)} = (0.2, 0.2, 0.4)
\]</span> and transition probability matrix <span class="math display">\[
    \boldsymbol{P} =
    \begin{pmatrix}
        0.8 &amp; 0.1 &amp; 0.1 \\
        0.3 &amp; 0.7 &amp; 0 \\
        0 &amp; 0.2 &amp; 0.8
    \end{pmatrix}
\]</span></p>
<p>This outputs one realisation from the process, and changing the random seed would yield a different realisation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">67</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>u0 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>              <span class="fl">0.3</span>, <span class="fl">0.7</span>, <span class="dv">0</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>              <span class="dv">0</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialise</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="at">length =</span> n)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> u0)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop over time steps</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n) {</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Choose row of transition matrix based on previous state </span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    P_row <span class="ot">&lt;-</span> P[X[i<span class="dv">-1</span>],]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample new state</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    X[i] <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> P_row)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Minus 1 to get states {0, 1, 2} rather than {1, 2, 3}</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>X <span class="sc">-</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [1] 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 2 2 2 2 2 2 2 1 1 1 1 1 1
 [38] 1 1 1 1 1 0 0 0 0 2 2 2 2 2 2 2 2 1 0 0 0 0 2 2 1 1 1 1 1 1 1 1 1 0 1 1 1
 [75] 0 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</code></pre>
</div>
</div>
<p>Simulation is an extremely flexible and powerful tool in applied probability. In the rest of this chapter, we will describe many mathematical techniques to answer questions such as</p>
<ul>
<li><p>how often does the chain visit state <span class="math inline">\(i\)</span>?</p></li>
<li><p>if the chain starts in state <span class="math inline">\(i\)</span>, how long will it take on average to reach state <span class="math inline">\(j\)</span>?</p></li>
<li><p>if the chain is in state <span class="math inline">\(i\)</span> at time <span class="math inline">\(t = 0\)</span>, what is the probability that it will be in state <span class="math inline">\(j\)</span> at time <span class="math inline">\(t = 10\)</span>?</p></li>
</ul>
<p>Approximate answers to all these questions, and many more, could be obtained based on simulated realisations of the process.</p>
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
</section>
<section id="looking-into-the-future" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="looking-into-the-future"><span class="header-section-number">2.2</span> Looking into the future</h2>
<p>The transition probabilities describe what happens to the process over one time interval. With this information, it seems that we should also be able to say something about the distribution of the process further into the future (although perhaps with less and less certainty).</p>
<section id="chapman-kolmogorov-equations" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="chapman-kolmogorov-equations"><span class="header-section-number">2.2.1</span> Chapman-Kolmogorov Equations</h3>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.5
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong><span class="math inline">\(n\)</span>-step transition probability</strong> from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> is <span class="math display">\[
P_{ij}^{(n)} = \Pr(X_{m + n} = j \mid X_m = i),
\]</span> for any <span class="math inline">\(m \geq 0\)</span>. It is the probability that the process will be in state <span class="math inline">\(j\)</span> after <span class="math inline">\(n\)</span> transitions, given that it started in state <span class="math inline">\(i\)</span>.<br>
</p>
<p>The <span class="math inline">\(n\)</span>-step transition probability matrix is denoted as <span class="math display">\[
\boldsymbol{P}^{(n)} =
\begin{pmatrix}
P_{00}^{(n)} &amp; P_{01}^{(n)} &amp; P_{02}^{(n)} &amp; \cdots \\
P_{10}^{(n)} &amp; P_{11}^{(n)} &amp; P_{12}^{(n)} &amp; \cdots \\
P_{20}^{(n)} &amp; P_{21}^{(n)} &amp; P_{22}^{(n)} &amp; \cdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{pmatrix}
\]</span></p>
</div>
</div>
<p>Note that the transition probabilities that we defined previously are 1-step transition probabilities, i.e., we have <span class="math inline">\(\boldsymbol{P}^{(1)} = \boldsymbol{P}\)</span> and <span class="math inline">\(P^{(1)}_{ij} = P_{ij}\)</span>. For any <span class="math inline">\(n\)</span>, the <span class="math inline">\(n\)</span>-step transition probabilities can be derived from the <span class="math inline">\(1\)</span>-step transition probabilities, and the Chapman-Kolmogorov equations provide this relationship.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.2 (Chapman-Kolmogorov equations)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Chapman-Kolmogorov equations are <span id="eq-chapman"><span class="math display">\[
P_{ij}^{(n + m)} = \sum_{k \in \mathcal{S}} P_{ik}^{(n)} P_{kj}^{(m)}
\tag{2.1}\]</span></span> for any <span class="math inline">\(i, j \in \mathcal{S}\)</span>, and any <span class="math inline">\(n, m \in \mathbb{N}\)</span>.<br>
</p>
<p>Equivalently, in matrix notation: <span class="math display">\[
\boldsymbol{P}^{(n+m)} = \boldsymbol{P}^{(n)} \boldsymbol{P}^{(m)}.
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The Chapman-Kolmogorov equations can be viewed as an application of the law of total probability to the Markov chain.</p>
<p><span class="math display">\[
\begin{aligned}
P_{ij}^{(n+m)} &amp; = \Pr(X_{n+m} = j \mid X_0 = i) &amp;&amp;\\
&amp; = \sum_{k \in \mathcal{S}} \Pr(X_{n+m} = j, X_n = k \mid X_0 = i) &amp;&amp; \text{(a)} \\
&amp; = \sum_{k \in \mathcal{S}} \Pr(X_{n+m} = j \mid X_n = k, X_0 = i) \Pr(X_n = k \mid X_0 = i) &amp;&amp; \text{(b)} \\
&amp; = \sum_{k \in \mathcal{S}} \Pr(X_{n+m} = j \mid X_n = k) \Pr(X_n = k \mid X_0 = i) &amp;&amp; \text{(c)} \\
&amp; = \sum_{k \in \mathcal{S}} P_{kj}^{(m)} P_{ik}^{(n)} \\
&amp; = (\boldsymbol{P}^{(n)} \boldsymbol{P}^{(m)})_{ij}
\end{aligned}
\]</span> Step (a) is the law of total probability, (b) uses the definition of conditional probability, and (c) uses the Markov property.</p>
</div>
</div>
</div>
<p>In particular, we have <span class="math display">\[
\begin{aligned}
\boldsymbol{P}^{(2)} = \boldsymbol{P}^{(1)} \boldsymbol{P}^{(1)}
= \boldsymbol{P}^{1} \boldsymbol{P}^{1} = \boldsymbol{P}^{2}, \\
\boldsymbol{P}^{(3)} = \boldsymbol{P}^{(2)} \boldsymbol{P}^{(1)}
= \boldsymbol{P}^{2} \boldsymbol{P}^{1} = \boldsymbol{P}^{3}, \\
\end{aligned}
\]</span> and so on. By induction, we can prove the following result.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Corrolary
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <span class="math inline">\(n\)</span>-step transition probability matrix of a Markov chain is <span class="math display">\[
\boldsymbol{P}^{(n)} = \boldsymbol{P}^n
\]</span></p>
</div>
</div>
<p>That is, the <span class="math inline">\(n\)</span>-step transition probability matrix <span class="math inline">\(\boldsymbol{P}^{(n)}\)</span> can be computed by multiplying the transition probability <span class="math inline">\(\boldsymbol{P}\)</span> by itself <span class="math inline">\(n\)</span> times.</p>
<p>Note that this does <em>not</em> imply that <span class="math inline">\(P_{ij}^{(n)} = P_{ij}^{n}\)</span> for any <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. This is generally not the case, and the full matrix needs to be taken to the power of <span class="math inline">\(n\)</span> to obtain <span class="math inline">\(P_{ij}^{(n)}\)</span>.</p>
</section>
<section id="marginal-state-distribution" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="marginal-state-distribution"><span class="header-section-number">2.2.2</span> Marginal state distribution</h3>
<p>The transition probabilities of the Markov chain define the conditional distribution of the state <span class="math inline">\(X_n\)</span> given the state <span class="math inline">\(X_{n-1}\)</span>. In some cases, we are interested in the marginal distribution (i.e., unconditional distribution) of <span class="math inline">\(X_n\)</span>.</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a Markov chain <span class="math inline">\((X_n)\)</span> with state space <span class="math inline">\(\mathcal{S}\)</span>, we denote as <span class="math inline">\(\boldsymbol{u}^{(n)}\)</span> the marginal distribution of the random variable <span class="math inline">\(X_n\)</span>, i.e., <span class="math display">\[
u_i^{(n)} = \Pr(X_n = i),\quad \text{for } i \in \mathcal{S}.
\]</span></p>
<p>Note that we interpret <span class="math inline">\(\boldsymbol{u}^{(n)}\)</span> as a row vector (which will be important for calculations later). By definition of probability distributions, we have <span class="math inline">\(u_i^{(n)} \geq 0\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(\sum_{i \in \mathcal{S}} u_i^{(n)} = 1\)</span>.</p>
</div>
</div>
<p>To derive the marginal distribution of <span class="math inline">\(X_n\)</span>, we must fix the initial distribution of the chain, <span class="math inline">\(\boldsymbol{u}^{(0)}\)</span>. In practice, we often know what the initial value of the process is, so the initial distribution is set to a vector where all but one entries are zero.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.3
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\((X_n)\)</span> be a Markov chain with initial distribution <span class="math inline">\(\boldsymbol{u}^{(0)}\)</span> and transition probability matrix <span class="math inline">\(\boldsymbol{P}\)</span>. For all <span class="math inline">\(n \geq 0\)</span>, the marginal distribution of <span class="math inline">\(X_n\)</span> is <span class="math inline">\(\boldsymbol{u}^{(n)} = \boldsymbol{u}^{(0)} \boldsymbol{P}^n\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{aligned}
u_j^{(n)} &amp; = \Pr(X_n = j) &amp;&amp;\\
&amp; = \sum_{i \in \mathcal{S}} \Pr(X_n = j \mid X_0 = i)\ \Pr(X_0 = i) &amp;&amp; \text{(a)} \\
&amp; = \sum_{i \in \mathcal{S}} P_{ij}^{(n)} u_i^{(0)} &amp;&amp; \text{(b)} \\
&amp; = \sum_{i \in \mathcal{S}} P_{ij}^{n} u_i^{(0)} &amp;&amp; \text{(c)} \\
&amp; = (\boldsymbol{u}^{(0)} \boldsymbol{P}^{n})_j, &amp;&amp; (d)
\end{aligned}
\]</span> where (a) is the law of total probability, (b) uses the notation we have introduced for the marginal distribution and the <span class="math inline">\(n\)</span>-step transition probabilities, (c) follows from the Chapman-Kolmogorov equations, and (d) is an equivalent matrix operation.</p>
</div>
</div>
</div>
<p>From this property, we can see that the Markov chain is fully specified by the initial distribution <span class="math inline">\(\boldsymbol{u}^{(0)}\)</span> and the transition probability matrix <span class="math inline">\(\boldsymbol{P}\)</span>. That is, given those two parameters, the distribution of the chain can be computed at any time step <span class="math inline">\(n \geq 0\)</span>.</p>
<p>To illustrate the idea of marginal distribution, we use the Markov chain defined by random moves of a piece on a chess board. The state space of the process is the list of squares on the board, i.e., there are <span class="math inline">\(8 \times 8 = 64\)</span> states, and the <span class="math inline">\(64 \times 64 = 4096\)</span> transition probabilities are defined by the rules for the chosen piece. We assume that the piece is moved at each time step to any of the allowed squares with equal probability. For example, <a href="#fig-chess-tpm" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-chess-tpm</span></a> shows the transition probability matrices for a king and a knight.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-chess-tpm" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chess-tpm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-chess-tpm-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chess-tpm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: Visualisation of transition probability matrices for king (left) and knight (right) on a chess board, where non-zero elements are shown in dark. Note that the way the squares of the board are ordered as states is arbitrary, so the matrix would look different if another convention was used.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Given the starting position of the piece, we can use the last proposition to compute its distribution on the board after one move, two moves, and so on. The distribution is a vector of the probabilities of being in the different squares of the board, which add up to 1. If we choose the square where the piece starts, the initial distribution is a vector of length 64, where 63 elements are set to zero (all except the starting position). Then, we iteratively multiply that vector by the transition probability matrix to obtain the subsequent distributions.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-chess-dist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chess-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-chess-dist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chess-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: Distribution of position on chess board in successive time steps, given some initial position and movement rules.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-chess-dist" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-chess-dist</span></a> shows the distributions of a king and a knight that start from some given square, and take an allowable move at random at each time step. The king has what we might call “diffusive” behaviour, and its distribution spreads over the board with time. After many time steps, it is almost equally likely to be in any of the non-edge squares of the board; the edge and corner squares are less likely because they are less connected. The distribution of the knight also spreads over the board with time, but it follows a different alternating pattern. This is because, due to its movement rules, a knight that’s on a black square has to move to a white square at the next time, whereas a knight that’s on a white square has to move to a black square. So, if the piece starts on a black square, all white squares have probability zero when <span class="math inline">\(n\)</span> is even, and all black squares have probability zero when <span class="math inline">\(n\)</span> is odd.</p>
<p>This example highlights several interesting phenomena that we will study in more detail in later sections. In particular, it seems like there is a key difference in the long-term behaviour of the Markov chain for the king and the knight: the distribution of the king stabilises as <span class="math inline">\(n\)</span> grows, whereas the distribution of the knight does not.</p>
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
</section>
<section id="first-step-analysis" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="first-step-analysis"><span class="header-section-number">2.3</span> First step analysis</h2>
<p>It is common to seek the probability that a Markov chain will ever reach a given state (or set of states), and the expected time it will take to get there. These quantities are called hitting probability and expected hitting time, respectively, and this area is called first step analysis. We will see that these can be found by solving linear equations involving the transition probabilities.</p>
<p>For example, looking at the transition graph below, we might wonder whether, starting from state 2, it is more likely that the process will eventually get stuck in 4 or in <span class="math inline">\(\{ 0, 1 \}\)</span>. Or how long we might expect it will take.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-markov-firststep" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-markov-firststep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-markov-firststep">graph LR
0((0)) --&gt;|1| 1((1))
1 --&gt;|1| 0
2((2)) --&gt;|1/3| 0
2 --&gt;|1/3| 1
2 --&gt;|1/3| 3((3))
3 --&gt;|1/2| 2
3 --&gt;|1/2| 4((4))
4 --&gt;|1| 4
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-markov-firststep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Example Markov chain.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.6
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a Markov chain <span class="math inline">\((X_n)\)</span> with state space <span class="math inline">\(\mathcal{S}\)</span>, and a subset <span class="math inline">\(A \subset \mathcal{S}\)</span>. We call (first) <strong>hitting time</strong> the first time at which the subset is reached by the chain, <span class="math display">\[
\tau_A = \text{min} \{ n \geq 0 : X_n \in A \}.
\]</span> If the chain never visits <span class="math inline">\(A\)</span> (i.e., if <span class="math inline">\(\{ n \geq 0 : X_n \in A \} = \emptyset\)</span>), then <span class="math inline">\(\tau_A = \infty\)</span>. The <strong>expected hitting time</strong> of <span class="math inline">\(A\)</span> given that the chain started from state <span class="math inline">\(i\)</span> is <span class="math display">\[
T_{iA} = E[\tau_A \mid X_0 = i].
\]</span></p>
</div>
</div>
<p>A related concept is the <strong>first return time</strong>, which is the first non-zero time at which a state (or set of states) is visited by the chain.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.7
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>hitting probability</strong> of the set <span class="math inline">\(A \subset \mathcal{S}\)</span> given that the chain started in state <span class="math inline">\(i\)</span> is <span class="math display">\[
f_{iA} = \Pr(\tau_A &lt; \infty \mid X_0 = i).
\]</span></p>
</div>
</div>
<p>The hitting probability is sometimes obviously 1; for example, in a Markov chain over <span class="math inline">\(\{ 0, 1 \}\)</span> where all transitions have non-zero probability, the probability of eventually visiting state 1 given we start in 0 is 1. In other cases, it is obviously 0, when there are no possible sequence of transitions leading from state <span class="math inline">\(i\)</span> to <span class="math inline">\(A\)</span>.</p>
<p>First step analysis is often used to study systems with absorbing classes, i.e., sets of states where the process can get “stuck”. <a href="#fig-markov-firststep" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-firststep</span></a> shows an example Markov chain with two absorbing classes: <span class="math inline">\(\{ 0, 1 \}\)</span> and <span class="math inline">\(\{ 4 \}\)</span>. Starting in state 2 or 3, it is non-trivial to determine the probability of ever visiting state 4, say.</p>
<p>We use the following property to compute hitting probabilities.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.4
</div>
</div>
<div class="callout-body-container callout-body">
<p>The hitting probabilities satisfy <span class="math display">\[
f_{iA} =
\begin{cases}
\sum_{j \in \mathcal{S}} P_{ij} f_{jA} &amp; \text{if } i \notin A \\
1 &amp; \text{if } i \in A
\end{cases}
\]</span></p>
</div>
</div>
<!-- This proof and the next are inspired by the Privault book -->
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{aligned}
f_{iA} &amp; = \Pr(\tau_A &lt; \infty \mid X_0 = i) \\
&amp; = \sum_{j \in \mathcal{S}} \Pr(\tau_A &lt; \infty \mid X_1 = j, X_0 = i) \Pr(X_1 = j \mid X_0 = i) &amp;&amp; \text{(a)} \\
&amp; = \sum_{j \in \mathcal{S}} P_{ij} \Pr(\tau_A &lt; \infty \mid X_1 = j) &amp;&amp; \text{(b)} \\
&amp; = \sum_{j \in \mathcal{S}} P_{ij} \Pr(\tau_A &lt; \infty \mid X_0 = j) &amp;&amp; \text{(c)} \\
&amp; = \sum_{j \in \mathcal{S}} P_{ij} f_{jA}
\end{aligned}
\]</span> where (a) is the law of total probability, (b) is the Markov property, and (c) follows from the fact that the probability of eventually reaching <span class="math inline">\(A\)</span> does not depend on whether we start at time 0 or 1.</p>
</div>
</div>
</div>
<p>This proof helps us understand why this is called “first step analysis”: to find the hitting probabilities, we condition on the first step (i.e., on <span class="math inline">\(X_1\)</span>) and apply the law of total probability.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.4: calculating hitting probabilities
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider the Markov chain with transition graph shown in <a href="#fig-markov-firststep" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-firststep</span></a>. What is the probability that the process gets stuck in <span class="math inline">\(A = \{ 0, 1 \}\)</span> if it starts in state 2? What about if it starts in state 3?</p>
<p>The question here is to find the hitting probabilities <span class="math inline">\(f_{2A}\)</span> and <span class="math inline">\(f_{3A}\)</span>. We use the general formula, which simplifies because many transition probabilities are zero: <span class="math display">\[
\begin{aligned}
f_{2A} &amp; = \sum_{j = 0}^4 P_{2j} f_{jA} \\
&amp; = P_{20} f_{0A} + P_{21} f_{1A} + P_{23} f_{3A} \\
&amp; = \frac{1}{3} \times 1 + \frac{1}{3} \times 1 + \frac{1}{3} \times f_{3A}
\end{aligned}
\]</span></p>
<p>We proceed similarly for <span class="math inline">\(f_{3A}\)</span>: <span class="math display">\[
\begin{aligned}
f_{3A} &amp; = \sum_{j = 0}^4 P_{3j} f_{jA} \\
&amp; = P_{32} f_{2A} + P_{34} f_{4A} \\
&amp; = \frac{1}{2} \times f_{2A} + \frac{1}{2} \times 0
\end{aligned}
\]</span> where we use the fact that <span class="math inline">\(f_{4A} = \Pr(\tau_A &lt; \infty \mid X_0 = 4) = 0\)</span> because there are no transitions out of state 4.</p>
<p>We combine the two equations to find: <span class="math display">\[
\begin{aligned}
&amp; f_{2A} = \frac{2}{3} + \frac{1}{6} f_{2A} \\
\Rightarrow\quad &amp; \frac{5}{6} f_{2A} = \frac{2}{3} \\
\Rightarrow\quad &amp; f_{2A} = \frac{4}{5}
\end{aligned}
\]</span> and <span class="math display">\[
f_{3A} = \frac{1}{2} f_{2A} = \frac{2}{5}
\]</span></p>
</div>
</div>
</div>
<p>We now turn to the mean (or expected) hitting times, which can also be computed conveniently through a system of linear equations.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.5
</div>
</div>
<div class="callout-body-container callout-body">
<p>The expected hitting times satisfy <span class="math display">\[
T_{iA} =
\begin{cases}
1 + \sum_{j \in \mathcal{S}} P_{ij} T_{jA} &amp; \text{if } i \notin A \\
0 &amp; \text{if } i \in A.
\end{cases}
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{aligned}
T_{iA} &amp; = E[\tau_A \mid X_0 = i] \\
&amp; = \sum_{j \in \mathcal{S}} E[\tau_A \mid X_1 = j, X_0 = i] \Pr(X_1 = j \mid X_0 = i) &amp;&amp; \text{(a)}\\
&amp; = \sum_{j \in \mathcal{S}} P_{ij} E[\tau_A \mid X_1 = j] &amp;&amp; \text{(b)}\\
&amp; = \sum_{j \in \mathcal{S}} P_{ij} \left\{ 1 + E[\tau_A \mid X_0 = j] \right\} &amp;&amp; \text{(c)}\\
&amp; = \sum_{j \in \mathcal{S}} P_{ij} + \sum_{j \in \mathcal{S}} P_{ij} E[\tau_A \mid X_0 = j] \\
&amp; = 1 + \sum_{j \in \mathcal{S}} P_{ij} T_{jA}
\end{aligned}
\]</span> where (a) is the law of total expectation, (b) is the Markov property, and (c) states that it takes one more time step to reach <span class="math inline">\(A\)</span> in total if we start in state <span class="math inline">\(j\)</span> at time 1 rather than at time 0.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.5: calculating expected hitting times
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For the Markov chain in <a href="#fig-markov-firststep" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-firststep</span></a>, what it the expected time it takes the process to reach <span class="math inline">\(A = \{ 0, 1, 4 \}\)</span> if it starts in state 2? If it starts in state 3?</p>
<p>We can use the previous result to compute the two expected hitting times <span class="math inline">\(T_{2A}\)</span> and <span class="math inline">\(T_{3A}\)</span>. We have <span class="math display">\[
T_{2A} = 1 + \sum_{j \in \mathcal{S}} P_{2j} T_{jA}
\]</span> where <span class="math inline">\(T_{0A} = T_{1A} T_{4A} = 0\)</span> (because <span class="math inline">\(0, 1, 4 \in A\)</span>), and <span class="math inline">\(P_{22} = 0\)</span>, so <span class="math display">\[
T_{2A} = 1 + P_{23} T_{3A} = 1 + \frac{1}{3} T_{3A}
\]</span></p>
<p>Likewise, <span class="math display">\[
T_{3A} = 1 + P_{32} T_{2A} = 1 + \frac{1}{2} T_{2A}
\]</span></p>
<p>We combine the two equations, <span class="math display">\[
\begin{aligned}
&amp; T_{2A} = 1 + \frac{1}{3} \left( 1 + \frac{1}{2} T_{2A} \right) \\
\Rightarrow\quad &amp; T_{2A} = 1 + \frac{1}{3} + \frac{1}{6} T_{2A} \\
\Rightarrow\quad &amp; \frac{5}{6} T_{2A} = \frac{4}{3} \\
\Rightarrow\quad &amp; T_{2A} = \frac{8}{5}
\end{aligned}
\]</span> and <span class="math display">\[
T_{3A} = 1 + \frac{1}{2} \times \frac{8}{5} = \frac{9}{5}
\]</span></p>
</div>
</div>
</div>
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
<section id="interstate-travel" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="interstate-travel"><span class="header-section-number">2.4</span> Interstate travel</h2>
<!---
  https://bookdown.org/probability/beta/markov-chains.html
--->
<p>To describe the long-term behaviour of a Markov chain, we need to understand how states are related, i.e., how often the process travels from one state to another (not just over one time interval). In this section, we introduce several definitions that will become important in the next section to decide how the distribution of a given Markov chain evolves in the long run.</p>
<section id="communication-and-reducibility" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="communication-and-reducibility"><span class="header-section-number">2.4.1</span> Communication and reducibility</h3>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.8
</div>
</div>
<div class="callout-body-container callout-body">
<p>We say that state <span class="math inline">\(j\)</span> is <strong>accessible</strong> from state <span class="math inline">\(i\)</span> if <span class="math inline">\(P^{(n)}_{ij} &gt; 0\)</span> for some <span class="math inline">\(n \geq 0\)</span>, i.e., if there is positive probability of travelling from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in a finite number of steps.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.9
</div>
</div>
<div class="callout-body-container callout-body">
<p>We say that two states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> <strong>communicate</strong> if <span class="math inline">\(i\)</span> is accessible from <span class="math inline">\(j\)</span> and <span class="math inline">\(j\)</span> is accessible from <span class="math inline">\(i\)</span>, and we denote this as <span class="math inline">\(i \leftrightarrow j\)</span>.</p>
</div>
</div>
<p>The relation of communication satisfies the following three properties.</p>
<ol type="1">
<li><p>Reflexivity: Every state communicates with itself.</p></li>
<li><p>Symmetry: If state <span class="math inline">\(i\)</span> communicates with state <span class="math inline">\(j\)</span>, then <span class="math inline">\(j\)</span> communicates with <span class="math inline">\(i\)</span>.</p></li>
<li><p>Transitivity: If state <span class="math inline">\(i\)</span> communicates with state <span class="math inline">\(j\)</span>, and state <span class="math inline">\(j\)</span> communicates with state <span class="math inline">\(k\)</span>, then <span class="math inline">\(i\)</span> communicates with <span class="math inline">\(k\)</span>.</p></li>
</ol>
<p>A binary relation that satisfies these three properties is called an equivalence relation, and it can be used to divide the state space into equivalence classes.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.10
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two states that communicate are in the same <strong>class</strong>. This creates a partition of the state space into communicating classes.</p>
</div>
</div>
<p>The transition graph of a Markov process can be used to identify communicating classes.</p>
<p><strong>Example:</strong> <a href="#fig-markov-communication" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-communication</span></a> shows a 5-state Markov process with states {A, B, C, D, E}, where the transition probability matrix is <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0.5\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
\end{pmatrix}
\]</span></p>
<p>This Markov process has two communicating classes: {A, B, C} and {D, E}. A and E are not in the same class because, while it is possible to travel from A to E (through B and C), it is not possible to travel from E to A.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-markov-communication" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-markov-communication-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-markov-communication">graph LR
A((A)) --&gt; B((B))
B --&gt; C((C))
C --&gt; A
D((D)) --&gt; E((E))
E --&gt; D
C --&gt; E
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-markov-communication-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: Illustration of communicating classes.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.11
</div>
</div>
<div class="callout-body-container callout-body">
<p>A chain is <strong>irreducible</strong> if it only has one class, i.e., if all states communicate.</p>
</div>
</div>
<p>We will often focus on irreducible Markov chains later, when we study their long-term behaviour. It is a little more complicated to think about it for reducible processes, where we have to figure out which class the process will get stuck in (e.g., {E, D} in <a href="#fig-markov-communication" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-communication</span></a>), how long we can expect it will take to get stuck there, and so on. These are also problems that have been studied, but we will not cover them in this course.</p>
<p>A state <span class="math inline">\(i \in \mathcal{S}\)</span> is called an <strong>absorbing state</strong> if <span class="math inline">\(P_{ii} = 1\)</span>, i.e., if the process that has reached <span class="math inline">\(i\)</span> can never leave. If a Markov chain has an absorbing state, then it is not irreducible, because it is not possible to travel from the absorbing state to any other state. For example, we can represent the board game Snakes and Ladders as a Markov chain, in which the final square is an absorbing state (because the player stays there once they’ve reached it).</p>
</section>
<section id="transience-and-recurrence" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="transience-and-recurrence"><span class="header-section-number">2.4.2</span> Transience and recurrence</h3>
<p>We may want to know whether, starting in a given state, the process will ever return to it. We define the <strong>first return time</strong> in state <span class="math inline">\(i\)</span> as the first positive time at which the state is visited by the chain; that is, we define <span class="math inline">\(\rho_i = \min \{\, n \geq 1 : X_n = i \,\}\)</span>, and set <span class="math inline">\(\rho_i = \infty\)</span> if <span class="math inline">\(X_n \neq i\)</span> for all <span class="math inline">\(n \geq 1\)</span>. Further, let <span class="math inline">\(h_i\)</span> be the <strong>return probability</strong>, i.e., the probability that, starting in state <span class="math inline">\(i\)</span>, the chain will ever return to state <span class="math inline">\(i\)</span>, i.e., <span class="math display">\[
h_i = \Pr(\rho_i &lt; \infty \mid X_0 = i).
\]</span></p>
<p>Note that these terms are slightly different from those used in first step analysis: the first return time cannot be zero, whereas the first hitting time can.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.12
</div>
</div>
<div class="callout-body-container callout-body">
<p>State <span class="math inline">\(i\)</span> is called <strong>recurrent</strong> if <span class="math inline">\(h_i = 1\)</span>, i.e., if the process will return to state <span class="math inline">\(i\)</span> with probability 1.<br>
</p>
<p>The state is called <strong>transient</strong> if <span class="math inline">\(h_i &lt; 1\)</span>, i.e., if there is a non-zero probability that the process will never visit <span class="math inline">\(i\)</span> again.</p>
</div>
</div>
<p><strong>Example:</strong> In the Markov process with transition graph shown in <a href="#fig-recurrent-transient" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-recurrent-transient</span></a>, A and B are transient states, and C, D and E are recurrent state. If the process starts in A or B, there is a non-zero probabilility that it will never return (if it transitions to C). If it starts in C, D or E, the process will revisit the state with probability 1.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-recurrent-transient" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-recurrent-transient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-recurrent-transient">graph LR
A((A)) --&gt; B((B))
B --&gt; A
B --&gt; C((C))
C --&gt; D((D))
C --&gt; E((E))
E --&gt; D
D --&gt; C
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-recurrent-transient-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: Illustration of recurrent (C, D, E) and transient (A, B) states.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can show that a recurrent state <span class="math inline">\(i\)</span> will be visited infinitely many times by the process, if it starts in <span class="math inline">\(i\)</span>. Indeed, in that case, there is a probability 1 that the process will return to state <span class="math inline">\(i\)</span> after some number of transitions, by definition of a recurrent state. Once it returns to <span class="math inline">\(i\)</span>, the process is back to where it started, and again there is a probability 1 that the process will visit <span class="math inline">\(i\)</span> a third time. We can repeat this reasoning to show that the process will infinitely return to any recurrent state <span class="math inline">\(i\)</span>.</p>
<p>We can rewrite this statement in terms of transition probabilities. For any <span class="math inline">\(n \geq 0\)</span>, define the indicator variable <span class="math display">\[
I_n =
\begin{cases}
1 &amp; \text{if } X_n = i,\\
0 &amp; \text{otherwise,}
\end{cases}
\]</span> such that the total number of time steps spent in state <span class="math inline">\(i\)</span> is <span class="math inline">\(\sum_{n=0}^\infty I_n\)</span>. Then, <span class="math display">\[
\begin{aligned}
E \left[ \left. \sum_{n = 0}^\infty I_n \,\right|\, X_0 = i \right] &amp; =
\sum_{n=0}^\infty E \left[ I_n \mid X_0 = i \right] \\
&amp; = \sum_{n = 0}^\infty \Pr(X_n = i \mid X_0 = i) \\
&amp; = \sum_{n=0}^\infty P_{ii}^{(n)}
\end{aligned}
\]</span></p>
<p>This leads to an alternative definition for recurrence and transience.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.6
</div>
</div>
<div class="callout-body-container callout-body">
<p>State <span class="math inline">\(i\)</span> is recurrent if and only if <span class="math inline">\(\displaystyle \sum_{n=1}^\infty P_{ii}^{(n)} = \infty\)</span>.<br>
</p>
<p>State <span class="math inline">\(i\)</span> is transient if and only if <span class="math inline">\(\displaystyle \sum_{n=1}^\infty P_{ii}^{(n)} &lt; \infty\)</span>.</p>
</div>
</div>
<p>Note that, when the state space is finite, transient states only exist for reducible Markov chains, i.e., when there is no possible path from one state to another. However, when the state space is (countably) infinite, it is possible to have a transient state in an irreducible chain.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.6: transient random walk
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In an irreducible transient Markov chain, there is a positive probability of travelling from any state to any state, but there is also a positive probability of never visiting any given state again. One example is the Markov chain over <span class="math inline">\(\mathcal{S} = \mathbb{Z}\)</span>, where, for each state <span class="math inline">\(i\)</span>, <span class="math display">\[
\begin{cases}
\Pr(X_{n+1} = i + 1 \mid X_n = i) = p \\
\Pr(X_{n+1} = i - 1 \mid X_n = i) = q
\end{cases}
\]</span> if <span class="math inline">\(p \neq q \neq 0.5\)</span>.</p>
<p>That is, at each time step, the process has probability <span class="math inline">\(p\)</span> of moving one unit to the right, and probability <span class="math inline">\(q\)</span> of moving one unit to the left.</p>
<p>All states communicate, because we can in principle go between any two states in <span class="math inline">\(\mathcal{S}\)</span>, so this is an irreducible Markov chain. However, because there is a higher probability of moving in one direction, the chain will move in that direction on average, and there is a positive probability that it will never visit 0 again, for example. A realisation from this chain over 100 time steps is shown in <a href="#fig-ex-transience" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ex-transience</span></a>, for <span class="math inline">\(p = 0.8\)</span> and <span class="math inline">\(q = 0.2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ex-transience" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ex-transience-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-ex-transience-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ex-transience-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9: Simulated realisation from example transient Markov chain.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
<p>We end this section with a convenient property, which will allow us to talk about the transience and recurrence of communication classes, rather than individual states.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.7
</div>
</div>
<div class="callout-body-container callout-body">
<!---
 Corollary 4.2 of Ross (p207)  
--->
<p>Transience and recurrence are class properties:</p>
<ol type="1">
<li><p>if <span class="math inline">\(i\)</span> is transient and <span class="math inline">\(i \leftrightarrow j\)</span>, then <span class="math inline">\(j\)</span> is transient;</p></li>
<li><p>if <span class="math inline">\(i\)</span> is recurrent and <span class="math inline">\(i \leftrightarrow j\)</span>, then <span class="math inline">\(j\)</span> is recurrent.</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Assume that <span class="math inline">\(i\)</span> is recurrent and <span class="math inline">\(i \leftrightarrow j\)</span>. Because there is a positive probability of travelling between the two states, then we must have <span class="math inline">\(P_{ij}^{(k)} &gt; 0\)</span> and <span class="math inline">\(P_{ji}^{(m)}\)</span> for some <span class="math inline">\(k\)</span> and <span class="math inline">\(m\)</span>. We also have <span class="math display">\[
    P_{jj}^{(m+n+k)} \geq P_{ji}^{(m)} P_{ii}^{(n)} P_{ij}^{(k)}
\]</span> because both sides describe the probability of travelling from <span class="math inline">\(j\)</span> to <span class="math inline">\(j\)</span> in <span class="math inline">\(m+n+k\)</span> steps, but the right hand side is a special case (also going through <span class="math inline">\(i\)</span>), with smaller probability.</p>
<p>Summing over <span class="math inline">\(n\)</span>, <span class="math display">\[
    \sum_{n = 1}^\infty P_{jj}^{(m+n+k)} \geq P_{ji}^{(m)} P_{ij}^{(k)} \sum_{n = 1}^\infty P_{ii}^{(n)}
\]</span></p>
<p>We know that <span class="math inline">\(P_{ij}^{(k)} &gt; 0\)</span> and <span class="math inline">\(P_{ji}^{(m)}\)</span>, and <span class="math inline">\(\sum_{n = 1}^\infty P_{ii}^{(n)} &gt; \infty\)</span> because state <span class="math inline">\(i\)</span> is recurrent. So <span class="math display">\[
    \sum_{n = 1}^\infty P_{jj}^{(m+n+k)} = \infty,
\]</span> proving that state <span class="math inline">\(j\)</span> is recurrent.</p>
<p>This also implies that transience is a class property: if <span class="math inline">\(i\)</span> is transient and communicates with state <span class="math inline">\(j\)</span>, then <span class="math inline">\(j\)</span> cannot be recurrent (because that would imply <span class="math inline">\(i\)</span> is recurrent, which it is not). If <span class="math inline">\(j\)</span> is not recurrent, then it must be transient.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-33-contents" aria-controls="callout-33" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.6: transient random walk (continued)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-33" class="callout-33-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can show that all states of an asymmetric random walk on <span class="math inline">\(\mathbb{Z}\)</span> are transient. We will mostly use Proposition 2.6, i.e., we wish to show that <span class="math display">\[
    \sum_{n=1}^\infty P_{00}^{(n)} &lt; \infty
\]</span> (We focus on proving the transience of state 0 because all states communicate, so this implies transience of all states.)</p>
<p>The chain can only return to state 0 in an even number of time steps so, for any <span class="math inline">\(n \in \mathbb{N}\)</span>, <span class="math inline">\(P_{00}^{(2n+1)} = 0\)</span>. The probability of returning to state 0 in <span class="math inline">\(2n\)</span> steps is <span class="math display">\[
    P_{00}^{(2n)} = \binom{2n}{n} p^n q^n
\]</span> because there are <span class="math inline">\(\binom{2n}{n}\)</span> combinations of right and left transitions it can take to reach 0 again, each with a total of <span class="math inline">\(n\)</span> steps to the right and <span class="math inline">\(n\)</span> steps to the left.</p>
<p>So we have <span class="math display">\[
\begin{aligned}
    \sum_{n = 0}^{\infty} P_{00}^{(n)} &amp; = \sum_{n = 0}^{\infty} P_{00}^{(2n)} \\
        &amp; = \sum_{n = 0}^\infty \binom{2n}{n} p^n q^n\\
        &amp; = \sum_{n = 0}^\infty (pq)^n \frac{1 \times 2 \times \cdots \times (2n - 2) \times (2n - 1) \times 2n}{(n!)^2} \\
        &amp; = \sum_{n = 0}^\infty (pq)^n \frac{2^{2n} \times 1/2 \times 2/2 \times \cdots \times (2n-1)/2 \times (2n)/2}{(n!)^2} \\
        &amp; = \sum_{n = 0}^\infty (4pq)^n \frac{(1/2) \times 1 \times (3/2) \times 2 \times \cdots \times (2n - 3)/2 \times (n-1) \times (2n - 1)/2 \times n}{(n!)^2} \\
        &amp; = \sum_{n = 0}^\infty (4pq)^n \frac{1/2 \times 3/2 \times \cdots \times (2n-3)/2 \times (2n - 1)/2}{n!} \\
        &amp; = \sum_{n = 0}^\infty (-1)^n (4pq)^n \frac{(-1/2) \times (-3/2) \times \cdots \times (3 - 2n)/2 \times (1- 2n)/2}{n!} \\
        &amp; =  \sum_{n = 0}^\infty (-4pq)^n \frac{(-1/2)(-1/2-1)(-1/2-2)\cdots(-1/2-n+1)}{n!} \\
        &amp; =  \sum_{n = 0}^\infty \binom{-1/2}{n} (-4pq)^n
\end{aligned}
\]</span> where <span class="math inline">\(\binom{-1/2}{n}\)</span> is defined as a “generalised” binomial coefficent. This is a special case of the well-known binomial series. If <span class="math inline">\(\vert 4pq \vert &lt; 1\)</span> (as is the case when <span class="math inline">\(p \neq q\)</span>), this series converges to <span class="math display">\[
    \sum_{n = 0}^{\infty} P_{00}^{(n)} = (1 - 4pq)^{-1/2} &lt; \infty
\]</span> as required. All states are therefore transient.</p>
</div>
</div>
</div>
</section>
<section id="different-types-of-recurrence" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="different-types-of-recurrence"><span class="header-section-number">2.4.3</span> Different types of recurrence</h3>
<p>We denote as <span class="math inline">\(m_j\)</span> the expected number of time steps it will take a chain that started in state <span class="math inline">\(j\)</span> to return to state <span class="math inline">\(j\)</span>, i.e., <span class="math display">\[
m_j = E[\rho_{j} \mid X_0 = j],
\]</span> where <span class="math inline">\(\rho_j\)</span> is the first return time defined in the previous section.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.13
</div>
</div>
<div class="callout-body-container callout-body">
<p>We say that a recurrent state <span class="math inline">\(j\)</span> is <strong>positive recurrent</strong> if <span class="math inline">\(m_j &lt; \infty\)</span>, and <strong>null recurrent</strong> if <span class="math inline">\(m_j = \infty\)</span>.</p>
</div>
</div>
<p>The distinction between positive and null recurrence only exists for Markov chains with infinite state spaces. If the state space is finite, then all recurrent states are positive recurrent. The notion of null recurrent state might be counter-intuitive: if there is positive probability that we revisit <span class="math inline">\(j\)</span>, then how can the expected time until this happens be infinite? (Note that there are many other such examples where expectations defy our intuition, like the <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox">St Petersburg paradox</a>). To understand, we can look at the definitions. We call state <span class="math inline">\(j\)</span> recurrent if <span class="math inline">\(f_j = 1\)</span> where <span class="math display">\[
\begin{aligned}
f_j &amp; = \Pr(\rho_j &lt; \infty \mid X_0 = j) \\
&amp; = \sum_{n = 0}^\infty \Pr(\rho_j = n \mid X_0 = j)
\end{aligned}
\]</span> On the other hand, the expected return time <span class="math inline">\(m_j\)</span> is <span class="math display">\[
\begin{aligned}
m_j &amp; = E[\rho_{j} \mid X_0 = j] \\
&amp; = \sum_{n = 0}^\infty n \Pr(\rho_j = n \mid X_0 = j)
\end{aligned}
\]</span></p>
<p>Whether <span class="math inline">\(\rho_j\)</span> is (almost surely) finite therefore does not tell us anything about its expected value. In fact, <span class="math inline">\(E[X] &lt; \infty\)</span> implies <span class="math inline">\(\Pr(X &lt; \infty) = 1\)</span>, but the converse is not true.</p>
<!-- There is an example of the fact mentioned above on p 19 of the Privault book. -->
<p>Just like for transience and recurrence, positive and null recurrence are class properties, so we can use those terms to refer to a class rather than just a state.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.8
</div>
</div>
<div class="callout-body-container callout-body">
<!---
Proposition 4.5 of Ross 2019 (p217). 
--->
<p>Let <span class="math inline">\(i, j \in \mathcal{S}\)</span>.</p>
<ol type="1">
<li><p>If <span class="math inline">\(i\)</span> is a positive recurrent state and <span class="math inline">\(i \leftrightarrow j\)</span>, then <span class="math inline">\(j\)</span> is positive recurrent.</p></li>
<li><p>If <span class="math inline">\(i\)</span> is a null recurrent state and <span class="math inline">\(i \leftrightarrow j\)</span>, then <span class="math inline">\(j\)</span> is null recurrent.</p></li>
</ol>
</div>
</div>
<p>When all states communicate, i.e., when the chain is irreducible, we can go one step further and use the terms to refer to the chain.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.14
</div>
</div>
<div class="callout-body-container callout-body">
<p>An irreducible Markov chain is called transient if at least one state is transient; it is called positive recurrent if at least one state is positive recurrent; and it is called null recurrent if at least one state is null recurrent.</p>
</div>
</div>
<p>In the definition above, saying that at least one state is transient or recurrent is equivalent to saying that every state is, because they are class properties.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-37-contents" aria-controls="callout-37" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.7: null recurrence
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-37" class="callout-37-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The textbook example of a null recurrent Markov chain is the symmetric random walk over <span class="math inline">\(\mathcal{S} = \mathbb{Z}\)</span>; for any <span class="math inline">\(i \in \mathbb{Z}\)</span>, the transition probabilities are <span class="math display">\[
\begin{cases}
\Pr(X_{n+1} = i + 1 \mid X_n = i) = 0.5 \\
\Pr(X_{n+1} = i - 1 \mid X_n = i) = 0.5
\end{cases}
\]</span></p>
<p>We can show that this chain is recurrent following the same derivation used for the transient (asymmetric) random walk from the previous example. In this case, we have <span class="math display">\[
    \sum_{n = 0}^{\infty} P_{00}^{(n)} = \sum_{n = 0}^\infty \binom{-1/2}{n} (-4pq)^n
\]</span> with <span class="math inline">\(p = q = 0.5\)</span>. This binomial series does not converge because <span class="math inline">\(\vert 4pq \vert = 1\)</span>, so <span class="math inline">\(\sum_{n = 0}^{\infty} P_{00}^{(n)} = \infty\)</span> and the chain is recurrent.</p>
<!-- Norris (p 38), Privault (p 80), Ross (p 218) all have different proofs of the null recurrence. -->
<p>However, it can also be shown that the expected return time is infinite, i.e., it is null recurrent. A simulated realisation from this process is shown in <a href="#fig-ex-null" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ex-null</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ex-null" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ex-null-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-ex-null-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ex-null-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10: Simulated realisation from example null recurrent Markov chain.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-38-contents" aria-controls="callout-38" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.8: positive recurrence
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-38" class="callout-38-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Any irreducible Markov chain with finite state space is positive recurrent. For an example of a positive recurrent chain with infinite state space, consider the chain over <span class="math inline">\(\mathcal{S} = \mathbb{N}\)</span> with transition probabilities <span class="math display">\[
\begin{cases}
\Pr(X_{n+1} = i + 1 \mid X_n = i) = 0.2 \\
\Pr(X_{n+1} = i - 1 \mid X_n = i) = 0.8
\end{cases}
\]</span> for any state <span class="math inline">\(i &gt; 0\)</span>, and, if <span class="math inline">\(i = 0\)</span>, there is a 0.8 probability of remaining at 0, and a 0.2 probability of switching to 1.<br>
</p>
<p>Because zero is a dead end in this chain, and because it is more likely to go down than up, it will not suffer from the same divergence issues that we saw in the transient and null recurrent examples. This is a positive recurrent chain, and <a href="#fig-ex-positive" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ex-positive</span></a> shows an example realisation.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ex-positive" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ex-positive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-ex-positive-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ex-positive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.11: Simulated realisation from example positive recurrent Markov chain.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="periodicity" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="periodicity"><span class="header-section-number">2.4.4</span> Periodicity</h3>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.15
</div>
</div>
<div class="callout-body-container callout-body">
<p>A state <span class="math inline">\(i\)</span> has <strong>period</strong> <span class="math inline">\(d\)</span> if the number of steps it takes the chain to return to <span class="math inline">\(i\)</span> can only be a multiple of <span class="math inline">\(d\)</span>. In other words, <span class="math inline">\(d = \text{gcd} \{ n \in \mathbb{N}_{&gt;0} : P_{ii}^{(n)} &gt; 0\}\)</span>.<br>
</p>
<p>We say that <span class="math inline">\(i\)</span> is <strong>periodic</strong> if <span class="math inline">\(d(i) &gt; 1\)</span>, and <strong>aperiodic</strong> if <span class="math inline">\(d(i) = 1\)</span>.</p>
</div>
</div>
<p>It turns out that periodicity is also a class property, and so we also use the term to refer to a communicating class, or to an irreducible Markov chain.</p>
<p><a href="#fig-period1" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-period1</span></a> shows an example periodic Markov chain. The only way to go from A to A is to go through B, C and D exactly once, so the period is 4. <a href="#fig-period2" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-period2</span></a> is the transition graph of a very similar Markov chain, but it has been modified by adding a non-zero probability of remaining in state B. Then, the period becomes 1 because the chain can take any number of steps to return to a state, i.e., the chain is aperiodic.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-period1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-period1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-period1">graph LR
A((A)) --&gt; B((B))
B --&gt; C((C))
C --&gt; D((D))
D --&gt; A
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-period1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.12: Periodic Markov chain with period 4.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-period2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-period2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-period2">graph LR
A((A)) --&gt; B((B))
B --&gt; C((C))
C --&gt; D((D))
D --&gt; A
B --&gt; B
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-period2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.13: Aperiodic Markov chain.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Another example is the difference between the king and knight in the chess example from Section 2.2.2. The king can take 1, 2, 3, or any number of steps to return to its initial position, whereas the knight can only return to a position after 2, 4, 6, or any even number of steps. The king’s process is aperiodic, whereas the knight’s is periodic (with period 2).</p>
<!---
 ::: {.callout-note icon="false"}
## Definition 2.16

State $i$ is called **ergodic** if it is positive recurrent and aperiodic. _[Actually, this definition is not universal, see https://math.stackexchange.com/questions/152491/is-ergodic-markov-chain-both-irreducible-and-aperiodic-or-just-irreducible. The first reply argues that ergodic is synonymous with irreducible. ]_
::: 
--->
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
</section>
<section id="long-run-properties" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="long-run-properties"><span class="header-section-number">2.5</span> Long-run properties</h2>
<p>We are often interested in the long-run properties of the Markov chain, such as stability (does the system always converge to some distribution?) and long-run proportions (how much time does the process spend in each state on average?). We will link these questions to the concept of stationary distribution, and show how they can be answered in practice. In this section, we focus on irreducible aperiodic Markov chains. In this situation, the states must be either all positive recurrent, all null recurrent, or all transient.</p>
<!-- I'm now following a structure similar to that of Section 6.4 of Grimmett & Stirzaker 2020 (Probability and Random Processes) for the stationarity and limit theorems below (but not for the long-run proportions, which they don't touch on). -->
<section id="stationary-distribution" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="stationary-distribution"><span class="header-section-number">2.5.1</span> Stationary distribution</h3>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.17
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\((X_n)\)</span> be a Markov chain with transition probability matrix <span class="math inline">\(\boldsymbol{P}\)</span> on <span class="math inline">\(\mathcal{S}\)</span>, and consider the probability distribution <span class="math inline">\(\boldsymbol{\pi}\)</span> on <span class="math inline">\(\mathcal{S}\)</span>. We say that <span class="math inline">\(\boldsymbol\pi\)</span> is a <strong>stationary distribution</strong> of <span class="math inline">\((X_n)\)</span> if <span class="math display">\[
\boldsymbol{\pi} \boldsymbol{P} = \boldsymbol{\pi}.
\]</span></p>
<p>Equivalently, <span class="math inline">\(\boldsymbol{\pi} = (\pi_0, \pi_1, \dots)\)</span> is a stationary distribution if it satisfies <span class="math display">\[
\sum_{i \in \mathcal{S}} \pi_i P_{ij} = \pi_j,\quad \text{for any } j \in \mathcal{S}.
\]</span></p>
</div>
</div>
<p>We use the term “stationary” because it represents an equilibrium. Indeed, if the initial distribution <span class="math inline">\(\boldsymbol{u}^{(0)}\)</span> is a stationary distribution of the chain, the distributions at times <span class="math inline">\(n = 1, 2, \dots\)</span> are <span class="math display">\[
\begin{aligned}
\boldsymbol{u}^{(1)} &amp; = \boldsymbol{u}^{(0)} \boldsymbol{P} = \boldsymbol{u}^{(0)}, \\
\boldsymbol{u}^{(2)} &amp; = \boldsymbol{u}^{(1)} \boldsymbol{P} = \boldsymbol{u}^{(0)} \boldsymbol{P}
= \boldsymbol{u}^{(0)}, \\
\boldsymbol{u}^{(3)} &amp; = \boldsymbol{u}^{(2)} \boldsymbol{P} = \boldsymbol{u}^{(0)} \boldsymbol{P}
= \boldsymbol{u}^{(0)},
\end{aligned}
\]</span> and so on. That is, if a Markov chain starts in its stationary distribution, then it will remain in the stationary distribution. The stationary distribution is also sometimes called the <strong>equilibrium</strong> or <strong>invariant</strong> distribution of the process.</p>
<p><a href="#fig-statdist-example" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-statdist-example</span></a> shows two transition graphs. Example 1 is a 2-state Markov chain with stationary distribution <span class="math inline">\(\pi = (0.5, 0.5)\)</span>: if there is a 50-50 chance of being in either state at time <span class="math inline">\(t\)</span>, it will remain the same at time <span class="math inline">\(t+1\)</span>. Example 2 is a 4-state Markov chain with no transitions into state <span class="math inline">\(0\)</span>, and its stationary distribution is <span class="math inline">\(\pi = (0, 1/3, 1/3, 1/3)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-statdist-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-statdist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-statdist-example">graph TD

subgraph Example 2
    0((0)) --&gt;|1| 1((1))
    1 --&gt;|1| 2((2))
    2 --&gt;|1| 3((3))
    3 --&gt; 1
end

subgraph Example 1
    A((0)) --&gt;|0.1| B((1))
    B --&gt;|0.1| A
    A --&gt;|0.9| A
    B --&gt;|0.9| B
end
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-statdist-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.14: Example Markov chains, to illustrate stationary distributions.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-41-contents" aria-controls="callout-41" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.9: Calculating the stationary distribution “by hand”
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-41" class="callout-41-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider the two-state Markov chain with transition probability matrix <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
1 - p_1 &amp; p_1 \\
p_2 &amp; 1 - p_2
\end{pmatrix}
\]</span> where <span class="math inline">\(0 &lt; p_1, p_2 &lt; 1\)</span>. Find its stationary distribution(s).<br>
</p>
<p>We are looking for a vector <span class="math inline">\(\boldsymbol{\pi} = (\pi_1, \pi_2)\)</span> that satisfies <span class="math display">\[
\begin{aligned}
&amp; \boldsymbol\pi \boldsymbol{P} = \boldsymbol\pi \\
\Leftrightarrow\quad &amp; \begin{pmatrix} \pi_1 &amp; \pi_2 \end{pmatrix}
\begin{pmatrix} 1 - p_1 &amp; p_1\\ p_2 &amp; 1 - p_2 \end{pmatrix} = \begin{pmatrix} \pi_1 &amp; \pi_2 \end{pmatrix} \\
\Leftrightarrow\quad &amp;
\begin{cases}
\pi_1 (1 - p_1) + \pi_2 p_2 = \pi_1 \\
\pi_1 p_1 + \pi_2 (1 - p_2) = \pi_2
\end{cases} \\
\Leftrightarrow\quad &amp;
\begin{cases}
p_1 \pi_1 = p_2 \pi_2 \\
p_1 \pi_1 = p_2 \pi_2
\end{cases} \\
\end{aligned}
\]</span> The two equations are redundant, so we only keep one of them, and use the constraint <span class="math inline">\(\pi_1 + \pi_2 = 1\)</span> to find the solution: <span class="math display">\[
\begin{aligned}
&amp; \begin{cases}
p_1 \pi_1 = p_2 \pi_2 \\
\pi_1 + \pi_2 = 1
\end{cases} \\
\Leftrightarrow\quad &amp;
\begin{cases}
p_1 \pi_1 = p_2 \pi_2 \\
\pi_2 = 1 - \pi_1
\end{cases} \\
\Leftrightarrow\quad &amp;
\begin{cases}
p_1 \pi_1 = (1 - \pi_1) p_2 \\
\pi_2 = 1 - \pi_1
\end{cases} \\
\Leftrightarrow\quad &amp;
\begin{cases}
\pi_1 (p_1 + p_2) = p_2 \\
\pi_2 = 1 - \pi_1
\end{cases} \\
\Leftrightarrow\quad &amp;
\boldsymbol\pi =
\begin{pmatrix}
\frac{p_2}{p_1 + p_2} &amp; \frac{p_1}{p_1 + p_2}
\end{pmatrix}
\end{aligned}
\]</span></p>
<p>So the Markov chain has a unique stationary distribution <span class="math inline">\(\boldsymbol\pi\)</span> given above.<br>
</p>
<p>In general, the derivation follows the same steps:</p>
<ol type="1">
<li><p>write the system of equations <span class="math inline">\(\boldsymbol\pi \boldsymbol{P} = \boldsymbol\pi\)</span>;</p></li>
<li><p>drop one of the equations;</p></li>
<li><p>use the row sum constraint to solve the system.<br>
</p></li>
</ol>
<p>We will see that there are more efficient methods to derive the stationary distribution, which are particularly useful for models with a large number of states.</p>
</div>
</div>
</div>
<p>What is the connection between the long-term behaviour of the process and its stationary distribution? The following proposition gives us a link with the limiting distribution of the process, when it exists.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.9
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assume that <span class="math inline">\(u_i^{(n)}\)</span> has a limit for <span class="math inline">\(n \to \infty\)</span> for all <span class="math inline">\(i \in \mathcal{S}\)</span>, and denote it as <span class="math display">\[
\pi_i = \lim_{n \to \infty} u_i^{(n)}.
\]</span></p>
<p>Then <span class="math inline">\(\boldsymbol{\pi} = (\pi_1, \pi_2, \dots)\)</span> is a stationary distribution of the chain.</p>
</div>
</div>
<!-- From Norris 1998 (p 33) -->
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-43-contents" aria-controls="callout-43" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-43" class="callout-43-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We will prove this proposition only in the case where <span class="math inline">\(\mathcal{S}\)</span> is finite, but it also applies to Markov chains with infinite state spaces.<br>
</p>
<p>We first show that <span class="math inline">\(\boldsymbol\pi\)</span> is a probability distribution over <span class="math inline">\(\mathcal{S}\)</span>. We have <span class="math display">\[
\begin{aligned}
\sum_{j \in \mathcal{S}} \pi_j &amp; = \sum_{j \in \mathcal{S}} \lim_{n \to \infty} u_j^{(n)} &amp;&amp; \\
&amp; = \lim_{n \to \infty} \sum_{j \in \mathcal{S}} u_j^{(n)} &amp;&amp; \text{(a)} \\
&amp; = 1, &amp;&amp; \text{(b)}
\end{aligned}
\]</span> where we can swap the limit and sum in (a) because <span class="math inline">\(\mathcal{S}\)</span> is finite, and (b) holds because <span class="math inline">\(\boldsymbol{u}^{(n)}\)</span> is a probability distribution.<br>
</p>
<p>Moreover, <span class="math display">\[
\begin{aligned}
\pi_j &amp; = \lim_{n \to \infty} \Pr(X_n = j) \\
&amp; = \lim_{n \to \infty} \Pr(X_{n + 1} = j) \\
&amp; = \lim_{n \to \infty} \sum_{i \in \mathcal{S}} \Pr(X_n = i) P_{ij} &amp;&amp; \text{(a)}\\
&amp; = \sum_{i \in \mathcal{S}} \lim_{n \to \infty}  \Pr(X_n = i) P_{ij} &amp;&amp; \text{(b)}\\
&amp; = \sum_{i \in \mathcal{S}} \pi_i P_{ij},
\end{aligned}
\]</span> where (a) is the law of total probability, and (b) follows from the finiteness of <span class="math inline">\(\mathcal{S}\)</span> as before. We can then conclude that <span class="math inline">\(\boldsymbol\pi\)</span> is a stationary distribution of the chain.</p>
</div>
</div>
</div>
<p>This result is not particularly useful in practice, because it assumes that we know what the limiting probabilities are, but it is a good starting point, and we will later see that we can go the other direction (from the stationary distribution, which we know how to compute, to the limiting probabilities).</p>
<p>The next theorem outlines the conditions under which an irreducible Markov chain has a stationary distribution, and the connection between the stationary distribution and the expected return time. Recall that the expected return time to state <span class="math inline">\(j\)</span> is defined as <span class="math inline">\(m_j = E[\rho_j \mid X_0 = j]\)</span>, where <span class="math inline">\(\rho_j = \min \{ n &gt; 0 : X_n = j\}\)</span> is the first return time to state <span class="math inline">\(j\)</span>.</p>
<!-- Grimmett & Strizaker (2020), Section 6.4, p254 -->
<!-- Norris, Theorem 1.7.7, p37 -->
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.10
</div>
</div>
<div class="callout-body-container callout-body">
<p>An irreducible Markov chain has a stationary distribution <span class="math inline">\(\boldsymbol{\pi}\)</span> if and only if it is positive recurrent. In this case, there is a unique stationary distribution, with elements <span class="math display">\[
\pi_i = \frac{1}{m_i},\quad \text{for } i \in \mathcal{S}.
\]</span></p>
</div>
</div>
<p>We can interpret the connection to the return time as follows: the longer it takes to return to state <span class="math inline">\(i\)</span>, the less time the chain will spend in state <span class="math inline">\(i\)</span> overall. If the process visits state <span class="math inline">\(i\)</span> every <span class="math inline">\(m_i\)</span> time steps, then it makes sense that the proportion of time spent at <span class="math inline">\(i\)</span> is <span class="math inline">\(1/m_i\)</span>.</p>
</section>
<section id="limiting-probabilities" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="limiting-probabilities"><span class="header-section-number">2.5.2</span> Limiting probabilities</h3>
<!-- Grimmett & Strizaker (2020), Section 6.4, p259 -->
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.11
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\((X_n)\)</span> be an irreducible aperiodic Markov chain with <span class="math inline">\(n\)</span>-step transition probabilities <span class="math inline">\(P_{ij}^{(n)}\)</span> for <span class="math inline">\(i, j \in \mathcal{S}\)</span>. Then, we have <span class="math display">\[
\lim_{n \to \infty} P_{ij}^{(n)} = \frac{1}{m_j},\quad \text{for } i, j \in \mathcal{S}.
\]</span></p>
<p>The limit does not depend on the starting state <span class="math inline">\(i\)</span> so, equivalently, we can write <span class="math display">\[
\lim_{n \to \infty} u_j^{(n)} = \frac{1}{m_j},\quad \text{for } j \in \mathcal{S},
\]</span> where <span class="math inline">\(\boldsymbol{u}^{(n)}\)</span> is the probability distribution of <span class="math inline">\(X_n\)</span>.</p>
</div>
</div>
<p>There are several important implications of this theorem for irreducible aperiodic chains:</p>
<ol type="1">
<li><p>If the chain is positive recurrent, then we saw in the previous section that <span class="math inline">\(\pi_j = 1/m_j\)</span> defines the unique stationary distribution of the process, so <span class="math inline">\(\lim_{n \to \infty} \boldsymbol{u}^{(n)} = \boldsymbol\pi\)</span>. In this case, the limiting distribution of <span class="math inline">\(X_n\)</span> and the stationary distribution coincide. This is very useful, because we usually know how to derive the stationary distribution of the process from the transition probability matrix.</p></li>
<li><p>If the chain is transient or null recurrent, then <span class="math inline">\(m_j = \infty\)</span>, and so <span class="math inline">\(\lim_{n \to \infty} u_{j}^{(n)} = 0\)</span> for all <span class="math inline">\(j \in \mathcal{S}\)</span>. The probability of being in any given state decreases to zero as time goes by. Note that this does not define a valid probability distribution for <span class="math inline">\(\boldsymbol{u}^{(n)}\)</span>; in this case, the process does not have a limiting distribution.</p></li>
</ol>
<p>In the previous theorem, we restricted our attention to aperiodic Markov chain because, in the case of periodic chains, the limits <span class="math inline">\(\lim_{n \to \infty} P_{ij}^{(n)}\)</span> and <span class="math inline">\(\lim_{n \to \infty} \Pr(X_n = i)\)</span> does not always exist. The following is a classic example of this situation.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-46-contents" aria-controls="callout-46" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.10: Periodic Markov chain
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-46" class="callout-46-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Convergence to equilibrium is only guaranteed when the chain is aperiodic. Consider the chain <span class="math inline">\((X_n)\)</span> with transition probability matrix <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix}
\]</span></p>
<p>Given some initial state <span class="math inline">\(X_0\)</span>, the evolution of the chain is deterministic: it switches state at each time step. <span class="math inline">\((X_n)\)</span> has <span class="math inline">\(\boldsymbol\pi = (0.5, 0.5)\)</span> as a stationary distribution, because <span class="math display">\[
\boldsymbol\pi \boldsymbol{P} =
\begin{pmatrix}0.5 &amp; 0.5\end{pmatrix}
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix}
= \boldsymbol\pi
\]</span> Intuitively, if there is a 50% probability of being in each state at time <span class="math inline">\(n\)</span>, there is still a 50% probability of being in each state at <span class="math inline">\(n + 1\)</span>.<br>
</p>
<p>However, we can show that there is no limiting distribution for this Markov chain, because <span class="math display">\[
\boldsymbol{P}^2 =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix},\quad
\boldsymbol{P}^3 =
\begin{pmatrix}
0 &amp; 1 \\
1 &amp; 0
\end{pmatrix},\quad
\boldsymbol{P}^4 =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix},\quad
\]</span> and so on. If <span class="math inline">\(X_0\)</span> is known, the state distribution <span class="math inline">\(\boldsymbol{u}^{(n)}\)</span> alternates between <span class="math inline">\((0, 1)\)</span> and <span class="math inline">\((1, 0)\)</span>, and so the limit <span class="math inline">\(\lim_{n \to \infty} u_i^{(n)}\)</span> does not generally exist. The only case where the limit exists is when <span class="math inline">\(\boldsymbol{u}^{(0)} = (0.5, 0.5)\)</span>, because this implies <span class="math inline">\(\boldsymbol{u}^{(n)} = (0.5, 0.5)\)</span> for any <span class="math inline">\(n\)</span>.</p>
</div>
</div>
</div>
<p>Another example is the difference between the random walk of a king and the knight on a chess board, as described in Section 2.2.2. We saw that the knight’s distribution over the board does not converge as time goes by, because it has a different limit for even and odd <span class="math inline">\(n\)</span> (leading to the alternating pattern in <a href="#fig-chess-dist" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-chess-dist</span></a>). This is because its position follows a periodic chain with period 2. On the other hand, the king’s chain is aperiodic, and it does have a limiting distribution.</p>
</section>
<section id="long-run-proportions" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="long-run-proportions"><span class="header-section-number">2.5.3</span> Long-run proportions</h3>
<!-- Definition similar to Norris book p53 -->
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition 2.18
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a Markov chain with state space <span class="math inline">\(\mathcal{S}\)</span>. For any <span class="math inline">\(i \in \mathcal{S}\)</span>, the <span class="math inline">\(i^\text{th}\)</span> <strong>long-run proportion</strong> is the proportion of time that the Markov chain spends in the <span class="math inline">\(i^\text{th}\)</span> state over the long run, i.e., <span class="math display">\[
\lim_{n \to \infty} \frac{1}{n} \sum_{k = 0}^{n-1} \mathbb{I}_{\{X_k = i\}},
\]</span> where <span class="math inline">\(\mathbb{I}\)</span> is the indicator function, i.e., <span class="math display">\[
\mathbb{I}_{\{X_k = i\}} =
\begin{cases}
1 &amp; \text{if } X_k = i \\
0 &amp; \text{otherwise.}
\end{cases}
\]</span></p>
</div>
</div>
<p>Note that the question of finding the long-run proportions of a chain, i.e., to know how often each state will be active if we let it run for a long time, is separate from the question addressed in the previous section. Indeed, even in cases where the chain does not have a limiting distribution, the long-run proportions might exist. In the example of a periodic Markov chain where <span class="math inline">\(p_{12} = p_{21} = 1\)</span>, we saw that there is no limiting distribution, but we can compute the proportion of time spent in each state. Because the chain switches at every time step, the proportion of time in each state will converge to 50%. Likewise, the knight moving randomly around a chess board does have long-term proportions, even though it doesn’t have a limiting distribution.</p>
<!-- Ross, Proposition 4.4 (p216), and Theorem 4.1 (p218) -->
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.12 (ergodic theorem)
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\((X_n)\)</span> is an irreducible Markov chain, then the long-run proportions converge almost surely and we have <span class="math display">\[
\Pr \left( \lim_{n \to \infty} \frac{1}{n} \sum_{k = 0}^{n-1} \mathbb{I}_{\{X_k = i\}} = \frac{1}{m_i} \right) = 1,\quad \text{for } i \in \mathcal{S}.
\]</span></p>
</div>
</div>
<p>This time, the result does not require aperiodicity. Based on results from the previous sections, this implies that, for an irreducible Markov chain:</p>
<ol type="1">
<li><p>for a positive recurrent chain, the long-run proportions coincide with the stationary distribution;</p></li>
<li><p>for a transient or null recurrent chain, the long-run proportions are zero.</p></li>
</ol>
<p>We will not prove this result, but we can provide some intuition. If <span class="math inline">\(\pi_i'\)</span> is the long-run proportion of time spent in state <span class="math inline">\(i\)</span>, then the long-run proportion of transitions that go from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> is <span class="math inline">\(\pi_i' P_{ij}\)</span>. If we sum over all <span class="math inline">\(i\)</span>, this becomes <span class="math display">\[
\pi'_j = \sum_{i \in \mathcal{S}} \pi'_i P_{ij}
\]</span> or, in matrix notation, <span class="math inline">\(\boldsymbol{\pi}' \boldsymbol{P} = \boldsymbol{\pi}'\)</span>. That is, <span class="math inline">\(\boldsymbol{\pi}'\)</span> is a stationary distribution of the Markov chain.</p>
</section>
<section id="calculating-the-stationary-distribution" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="calculating-the-stationary-distribution"><span class="header-section-number">2.5.4</span> Calculating the stationary distribution</h3>
<p>To compute the limiting distribution of a Markov chain or its long-run proportions, the most convenient approach is usually to find its stationary distribution (for irreducible, positive recurrent processes). We describe two approaches to compute the stationary distribution of a Markov chain with finite state space (when it exists), one based on the eigendecomposition of the transition probability matrix, and one that only requires inverting a well-chosen matrix.</p>
<p><strong>Method 1: eigendecomposition</strong></p>
<p>Recall that an eigenvector (or right eigenvector) of the square matrix <span class="math inline">\(\boldsymbol{A}\)</span> is a non-zero vector <span class="math inline">\(\boldsymbol{x}\)</span> such that <span class="math inline">\(\boldsymbol{Ax} = \lambda \boldsymbol{x}\)</span> for some scalar <span class="math inline">\(\lambda\)</span>; <span class="math inline">\(\lambda\)</span> is called the eigenvalue associated with <span class="math inline">\(\boldsymbol{x}\)</span>. This looks a little similar to the definition of a stationary distribution, except the stationary distribution is a <em>left</em> eigenvector of the transition probability matrix (i.e., we left-multiply the transition matrix). Most eigenanalysis theory and software focuses on right eigenvector but, fortunately, there is a close connection between right and left eigenvectors. Indeed, <span class="math inline">\(\boldsymbol{x}\)</span> is a right eigenvector of <span class="math inline">\(\boldsymbol{A}\)</span> if and only if <span class="math inline">\(\boldsymbol{x}^\intercal\)</span> is a left eigenvector of <span class="math inline">\(\boldsymbol{A}^\intercal\)</span>. You can see this using the general identity <span class="math inline">\((\boldsymbol{Ax})^\intercal = \boldsymbol{x}^\intercal \boldsymbol{A}^\intercal\)</span>.</p>
<p>Finding the stationary distribution of a Markov chain is therefore equivalent to finding the eigenvector of <span class="math inline">\(\boldsymbol{P}^\intercal\)</span> associated with the eigenvalue 1. Note that eigenvectors are defined up to a multiplicative constant, because any multiple of an eigenvector is also an eigenvector. So, once the eigenvector is found, we divide each element by the sum of all elements, so find the vector that defines a valid probability distribution over <span class="math inline">\(\mathcal{S}\)</span>.</p>
<p><strong>Method 2: matrix inverse</strong></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.13
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\((X_n)\)</span> be a Markov chain with transition probability matrix <span class="math inline">\(\boldsymbol{P}\)</span> and with <span class="math inline">\(\vert \mathcal{S} \vert = N\)</span> states. The probability distribution <span class="math inline">\(\boldsymbol\pi\)</span> is a stationary distribution of <span class="math inline">\((X_n)\)</span> if and only if <span class="math inline">\(\boldsymbol\pi(\boldsymbol{I} - \boldsymbol{P} + \boldsymbol{U}) = \boldsymbol{1}\)</span>, where</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{I}\)</span> is a <span class="math inline">\(N \times N\)</span> identity matrix;</p></li>
<li><p><span class="math inline">\(\boldsymbol{U}\)</span> is a <span class="math inline">\(N \times N\)</span> matrix of ones;</p></li>
<li><p><span class="math inline">\(\boldsymbol{1}\)</span> is a row vector of <span class="math inline">\(N\)</span> ones.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-50-contents" aria-controls="callout-50" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-50" class="callout-50-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We first show that <span class="math inline">\(\boldsymbol{\pi U} = \boldsymbol{1}\)</span>: <span class="math display">\[
\begin{aligned}
\boldsymbol{\pi U} &amp; =
\begin{pmatrix}
\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N
\end{pmatrix}
\begin{pmatrix}
1 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; 1 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; \cdots &amp; 1
\end{pmatrix} \\
&amp; = \begin{pmatrix}
\sum_k \pi_k &amp; \sum_k \pi_k &amp; \cdots &amp; \sum_k \pi_k
\end{pmatrix} \\
&amp; = \boldsymbol{1}
\end{aligned}
\]</span> where <span class="math inline">\(\sum_k \pi_k = 1\)</span> because <span class="math inline">\(\boldsymbol\pi\)</span> is a probability distribution.<br>
</p>
<p>We can then use this result to prove the proposition: <span class="math display">\[
\begin{aligned}
&amp; \boldsymbol\pi (\boldsymbol{I} - \boldsymbol{P} + \boldsymbol{U}) = \boldsymbol{1} \\
\Leftrightarrow\ &amp; \boldsymbol\pi \boldsymbol{I} - \boldsymbol\pi \boldsymbol{P} + \boldsymbol\pi\boldsymbol{U} = \boldsymbol{1} \\
\Leftrightarrow\ &amp; \boldsymbol\pi - \boldsymbol\pi \boldsymbol{P} + \boldsymbol{1} = \boldsymbol{1} \\
\Leftrightarrow\ &amp; \boldsymbol\pi - \boldsymbol\pi \boldsymbol{P} = \boldsymbol{0} \\
\Leftrightarrow\ &amp; \boldsymbol\pi = \boldsymbol\pi \boldsymbol{P},
\end{aligned}
\]</span> i.e., <span class="math inline">\(\boldsymbol{\pi}\)</span> is a stationary distribution of <span class="math inline">\((X_n)\)</span>.</p>
</div>
</div>
</div>
<p>When the stationary distribution exists, then <span class="math inline">\(\boldsymbol{I} - \boldsymbol{P} + \boldsymbol{U}\)</span> is invertible, and the stationary distribution can be calculated as <span class="math display">\[
\boldsymbol\pi = \boldsymbol{1} (\boldsymbol{I} - \boldsymbol{P} + \boldsymbol{U})^{-1}.
\]</span> In practice, the matrix to invert is straightforward to compute, and there are many methods to invert a matrix, so this is convenient to implement a general approach to find the stationary distribution of a chain with finite state space.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-51-contents" aria-controls="callout-51" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.11
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-51" class="callout-51-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We illustrate the implementation of the two proposed methods to compute the stationary distribution in R. This example uses a 3 by 3 transition probability matrix, but the code would be virtually identical for a general <span class="math inline">\(N\)</span> by <span class="math inline">\(N\)</span> matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Transition probability matrix</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.9</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>              <span class="dv">0</span>, <span class="fl">0.7</span>, <span class="fl">0.3</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>              <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.8</span>),</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="do">##################################</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Method 1: eigendecomposition ##</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="do">##################################</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get eigenvectors and eigenvalues</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>eig <span class="ot">&lt;-</span> <span class="fu">eigen</span>(<span class="fu">t</span>(P))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep vector associated with eigenvalue 1</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>eig_vec <span class="ot">&lt;-</span> eig<span class="sc">$</span>vectors[,<span class="fu">which</span>(<span class="fu">abs</span>(eig<span class="sc">$</span>values <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">&lt;</span> <span class="fl">1e-15</span>)]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalise vector to get a valid probability distribution</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>eig_vec <span class="sc">/</span> <span class="fu">sum</span>(eig_vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4 0.2 0.4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="do">################################</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Method 2: matrix inversion ##</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="do">################################</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define identity matrix and matrix of 1s</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>I <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">3</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>U <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># "solve" computes the matrix inverse, and "colSums" sums over columns</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colSums</span>(<span class="fu">solve</span>(I <span class="sc">-</span> P <span class="sc">+</span> U))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4 0.2 0.4</code></pre>
</div>
</div>
<p>The two methods return the same solution.</p>
</div>
</div>
</div>
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
</section>
<section id="statistical-inference" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="statistical-inference"><span class="header-section-number">2.6</span> Statistical inference</h2>
<p>So far, we have presented Markov chains as mathematical models, but we can also view them as data analysis tools. Given an observed sequence of states, which we assume to be a realisation from a Markov chain, we might want to estimate the transition probabilities of the assumed process. In turn, these can be used to determine its long-term properties.</p>
<p>We will not describe maximum likelihood estimation (MLE) in detail here. The likelihood is the joint probability (or probability density) of a sample of data under the model of interest, viewed as a function of the parameters of the model. MLE then consists of maximising the likelihood function with respect to the parameters, i.e., find the parameter values that are in best agreement with the observed data.</p>
<p>In this section, we first derive the likelihood function of a Markov chain, and then show how it can be maximised with respect to the transition probabilities to estimate them.</p>
<section id="likelihood-function" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="likelihood-function"><span class="header-section-number">2.6.1</span> Likelihood function</h3>
<p>Let <span class="math inline">\((X_n)\)</span> be a Markov chain with transition probability matrix <span class="math inline">\(\boldsymbol{P}\)</span>. For an observed sequence <span class="math inline">\(x_0, x_1, \ldots, x_n\)</span>, the likelihood function for <span class="math inline">\(\boldsymbol{P}\)</span> is given by the joint probability <span class="math inline">\(\Pr(X_0 = x_0, X_1 = x_1, \ldots, X_n = x_n)\)</span>. We can write <span class="math display">\[
\begin{aligned}
\Pr(X_0 = x_0, \ldots, X_n = x_n) = &amp;
\Pr(X_n = x_n \mid X_{n-1} = x_{n-1}, \dots, X_0 = x_0) \\
&amp; \quad \times \Pr(X_1 = x_1, \dots, X_{n-1} = x_{n-1}) \\
= &amp; \Pr(X_n = x_n \mid X_{n-1} = x_{n-1}) \\
&amp; \quad \times \Pr(X_0 = x_0, \dots, X_{n-1} = x_{n-1}) \\
\end{aligned}
\]</span> where the first equality uses the definition of conditional probability, and the second equality comes from the Markov property. Using the same reasoning, we also have <span class="math display">\[
\begin{aligned}
\Pr(X_0 = x_0, \ldots, X_{n-1} = x_{n-1}) = &amp;
\Pr(X_{n-1} = x_{n-1} \mid X_{n-2} = x_{n-2}, \dots, X_0 = x_0) \\
&amp; \quad \times \Pr(X_0 = x_0, \dots, X_{n-2} = x_{n-2}) \\
= &amp; \Pr(X_{n-1} = x_{n-1} \mid X_{n-2} = x_{n-2}) \\
&amp; \quad \times \Pr(X_0 = x_0, \dots, X_{n-2} = x_{n-2}) \\
\end{aligned}
\]</span> which we can combine with the previous equation to find <span class="math display">\[
\begin{aligned}
\Pr(X_0 = x_0, \ldots, X_n = x_n) = &amp;
\Pr(X_n = x_n \mid X_{n-1} = x_{n-1}) \\
&amp; \quad \times \Pr(X_{n-1} = x_{n-1} \mid X_{n-2} = x_{n-2}) \\
&amp; \quad \times \Pr(X_0 = x_0, \dots, X_{n-2} = x_{n-2}) \\
\end{aligned}
\]</span></p>
<p>We repeat this reasoning to find the joint probability as a product of transition probabilities <span class="math display">\[
\Pr(X_0 = x_0, \ldots, X_n = x_n) = \Pr(X_0 = x_0) \prod_{k=1}^n \Pr(X_k = x_k \mid X_{k-1} = x_{k-1})
\]</span></p>
<p>The first term, <span class="math inline">\(\Pr(X_0 = x_0)\)</span>, can either be treated as a parameter, or the first value can be viewed as deterministic, so <span class="math inline">\(\Pr(X_0 = x_0) = 1\)</span>. Here, we follow the latter approach, which does not affect inference on the transition probabilities.</p>
<p>Finally, the likelihood function is the joint probability of the observed data, written as a function of the parameter <span class="math inline">\(\boldsymbol{P}\)</span>: <span class="math display">\[
\begin{aligned}
L(\boldsymbol{P}) &amp; = \prod_{k=1}^n \Pr(X_k = x_k \mid X_{k-1} = x_{k-1}) \\
&amp; = \prod_{k=1}^n P_{x_{k-1}, x_k}
\end{aligned}
\]</span></p>
<p>It is convenient to notice that this product contains each transition probability <span class="math inline">\(P_{ij}\)</span> as many times as there are transitions from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in the observed sequence. For <span class="math inline">\(i, j \in \mathcal{S}\)</span>, let <span class="math inline">\(n_{ij}\)</span> be the number of transitions from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>. Then, <span class="math display">\[
L(\boldsymbol{P}) = \prod_{i \in \mathcal{S}} \prod_{j \in \mathcal{S}} P_{ij}^{n_{ij}}.
\]</span></p>
<p>We often compute the log-likelihood, rather than the likelihood itself, because it is often easier to work with analytically and numerically (e.g., see “Parameter estimation” below). Taking the log, we find <span class="math display">\[
\ell(\boldsymbol{P}) = \log [L(\boldsymbol{P})] = \sum_{i \in \mathcal{S}} \sum_{j \in \mathcal{S}} n_{ij} \log (P_{ij})
\]</span></p>
</section>
<section id="parameter-estimation" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="parameter-estimation"><span class="header-section-number">2.6.2</span> Parameter estimation</h3>
<p>The maximum likelihood estimator (MLE) of the transition probabilities in a Markov chain is obtained by counting the number of transitions between each pair of states in the observed data, and normalizing these counts to obtain the estimated probabilities.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition 2.14
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(n_{ij}\)</span> be the number of transitions from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in the observed data, and let <span class="math inline">\(n_i = \sum_j n_{ij}\)</span> be the total number of transitions from state <span class="math inline">\(i\)</span>. Then the MLE of the transition probability <span class="math inline">\(P_{ij}\)</span> from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> is given by <span class="math display">\[
\widehat{P}_{ij} = \frac{n_{ij}}{n_i}
\]</span></p>
</div>
</div>
<p>To prove that this estimator is indeed the maximum likelihood estimator, we need to show that it maximises the likelihood function of the observed data.</p>
<p>The MLE is obtained by maximizing <span class="math inline">\(L(\boldsymbol{P})\)</span> (or, equivalently, <span class="math inline">\(\ell(\boldsymbol{P})\)</span>) with respect to <span class="math inline">\(\boldsymbol{P}\)</span>, subject to the constraints that it is a stochastic matrix (i.e., entries are between 0 and 1, and rows sum to 1). This can be done using the method of Lagrange multipliers, which is a general approach to constrained optimisation problems. by noticing that each row of the transition probability matrix can be estimated separately. Specifically, we need to maximize the Lagrangian function <span class="math inline">\(\mathcal{L}\)</span> with respect to the transition probability of interest, where <span class="math display">\[
\mathcal{L}(\boldsymbol{P}, \lambda) = \ell(\boldsymbol{P}) + \sum_{i \in \mathcal{S}} \left\{ \lambda_i \left(\sum_{j \in \mathcal{S}} P_{ij} - 1\right) \right\}
\]</span> Taking the derivative of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(P_{ij}\)</span>, we obtain <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial P_{ij}} = \frac{n_{ij}}{P_{ij}} + \lambda_i.
\]</span></p>
<p>To find the maximum likelihood estimator <span class="math inline">\(\widehat{P}_{ij}\)</span>, we set the derivative to zero: <span class="math display">\[
\begin{aligned}
&amp; &amp; \frac{n_{ij}}{\widehat{P}_{ij}} + \lambda_i = 0 \\
\Rightarrow &amp; &amp; \widehat{P}_{ij} = - \frac{n_{ij}}{\lambda_i}
\end{aligned}
\]</span></p>
<p>We then use the row constraints on the transition probability matrix, and we find <span class="math display">\[
\begin{aligned}
&amp;&amp; \sum_{j \in \mathcal{S}} \widehat{P}_{ij} = 1 \\
\Rightarrow &amp;&amp; - \sum_{j \in \mathcal{S}} \frac{n_{ij}}{\lambda_i} = 1 \\
\Rightarrow &amp;&amp; \lambda_i = - \sum_{j \in \mathcal{S}} n_{ij}
\end{aligned}
\]</span></p>
<p>Finally, putting everything together, <span class="math display">\[
\widehat{P}_{ij} = \frac{n_{ij}}{\sum_{k \in \mathcal{S}} n_{ik}} = \frac{n_{ij}}{n_i}
\]</span></p>
<p>In practice, given a sequence of observed states, we can then find the “best fitting” Markov chain by calculating those transition probabilities from the numbers of transitions in the data. Based on the estimates, we can then simulate from the process, or compute the stationary distribution to better understand its long-term emerging features.</p>
<!---
 This estimator is consistent, meaning that it converges to the true transition probabilities as the sample size increases, and it is also asymptotically normal, meaning that its distribution approaches a normal distribution as the sample size increases. 
--->
<!-- ### Uncertainty quantification

*\[Parametric bootstrap? See Cosma Shalizi's note about this. Maybe I should run some simulations to see if the resulting confidence intervals have the correct coverage. Could this be an assignment, or does it require too much programming skills?\]* -->
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
</section>
<section id="markov-chains-with-uncountable-state-space" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="markov-chains-with-uncountable-state-space"><span class="header-section-number">2.7</span> Markov chains with uncountable state space</h2>
<!-- The StatLect website describes Markov chain first for finite state spaces, then countably infinite, and then uncountable, so it might be helpful to figure out what stays the same and what is different each time: statlect.com/fundamentals-of-statistics/Markov-chains -->
<p>We now briefly turn to the case where the state space is uncountable, e.g., <span class="math inline">\(\mathcal{S} = \mathbb{R}\)</span> or <span class="math inline">\(\mathcal{S} = [0, 1]\)</span>. Markov chains with uncountable state space have had great practical utility, as illustrated in the later section on Markov chain Monte Carlo methods.</p>
<p>A stochastic process <span class="math inline">\((X_n)\)</span> defined over an uncountable state space <span class="math inline">\(\mathcal{S}\)</span> is called a <strong>Markov process</strong> if <span class="math display">\[
f_{X_{n + 1} \mid X_{n}, \dots, X_{0}}(x_{n+1} \mid x_{n}, \dots, x_{0}) =
f_{X_{n + 1} \mid X_{n}}(x_{n+1} \mid x_{n})
\]</span></p>
<p>The idea is the same as for discrete state spaces: conditionally on the current state, future states are independent of past states. The only difference is that, now, the state takes on continuous rather than discrete values.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-53-contents" aria-controls="callout-53" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example 2.12: Gaussian random walk
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-53" class="callout-53-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider the process <span class="math inline">\((X_n)\)</span> defined by <span class="math inline">\(X_0 = 0\)</span> and <span class="math display">\[
X_{n+1} \mid X_n = x_n \sim N(x_n, 1),
\]</span> for <span class="math inline">\(n = 1, 2, \dots\)</span>. This process satisfies the Markov property, because the distribution of <span class="math inline">\(X_{n+1}\)</span> can be written in terms of only <span class="math inline">\(X_n\)</span>. Five example realisations of this Gaussian random walk are shown in <a href="#fig-gaussian-rw" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-gaussian-rw</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gaussian-rw" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gaussian-rw-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-gaussian-rw-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gaussian-rw-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.15: Five simulated realisations of Gaussian random walk starting at <span class="math inline">\(X_0 = 0\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
<p>In the uncountable case, the transition dynamics of the process cannot be written as a matrix (not even an infinite matrix). Instead, we define the transition kernel of the process, <span class="math inline">\(K : \mathcal{S} \times \mathcal{S} \rightarrow [0, \infty)\)</span>, as <span class="math display">\[
K(x, y) = f_{X_{n + 1} \mid X_{n}}(y \mid x),\quad \text{for } x, y \in \mathcal{S}.
\]</span> That is, <span class="math inline">\(K(x, y)\)</span> must be a probability density function with respect to <span class="math inline">\(y\)</span>, such that <span class="math inline">\(\int_{\mathcal{S}} K(x, y)\ dy = 1\)</span>. This function gives the probability density of each transition over continuous space. For example, in the Gaussian random walk described above, the transition kernel was given by the probability density function of the normal distribution.</p>
<p>Likewise, the initial distribution of the process must be written as a continuous probability distribution, with density function <span class="math inline">\(f_{X_0} : \mathcal{S} \to [0, \infty)\)</span>. The marginal distribution of the process at subsequent time points can then be derived recursively as <span class="math display">\[
\begin{aligned}
    f_{X_{n+1}}(x_{n+1}) = \int_{\mathcal{S}} f_{X_n}(x) K(x, x_{n+1})\ dx \\
\end{aligned}
\]</span></p>
<p>We say that the distribution <span class="math inline">\(\pi : \mathcal{S} \to [0, \infty)\)</span> is a stationary distribution of the Markov process <span class="math inline">\((X_n)\)</span> with transition kernel <span class="math inline">\(K\)</span> if <span class="math display">\[
    \int_{\mathcal{S}} \pi(x) K(x, y)\ dx = \pi(y).
\]</span> Because <span class="math inline">\(\pi\)</span> is a probability distribution over <span class="math inline">\(\mathcal{S}\)</span>, it is also subject to the constraint <span class="math inline">\(\int_{\mathcal{S}} \pi(x)\ dx = 1\)</span>. The problem of deriving the stationary distribution of a Markov chain over an uncountable state space is the foundation of Markov chain Monte Carlo, a powerful method to sample from complex probability distributions (described in the Applications section below).</p>
<p>The different types of Markov chains that we studied in the countable case have to be refined to the uncountable case, with the notions of <span class="math inline">\(\phi\)</span>-irreducibility, Harris recurrence, and periodicity over a partition of the state space, but we will not describe these here.</p>
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
<section id="applications" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="applications"><span class="header-section-number">2.8</span> Applications</h2>
<section id="markov-chain-monte-carlo" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="markov-chain-monte-carlo"><span class="header-section-number">2.8.1</span> Markov chain Monte Carlo</h3>
<p>Say that you want to generate a sample from a probability distribution with known probability density function, like the one in <a href="#fig-mcmc-dist" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-mcmc-dist</span></a>. For example, it might be challenging to derive the mean or quantiles of a complex distribution analytically, but those can easily be estimated from a simulated sample (i.e., approximated by the sample mean/quantiles). This problem turns out to have a very wide range of applications, in particular in the context of Bayesian inference.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mcmc-dist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mcmc-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-mcmc-dist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mcmc-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.16: Example probability density function that we would like to sample from.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Basic methods developed to sample from a given distribution include inverse transform sampling and rejection sampling, for example. These only work well in simple cases, e.g., when the cumulative distribution function can be evaluated, or for low-dimensional distributions.</p>
<p>Markov chain Monte Carlo (MCMC) is a general method which performs well in a wide range of situations, including for high-dimensional problems. The idea behind Markov chain Monte Carlo is to define a Markov chain over the parameter space, for which the stationary distribution is known to be the distribution of interest (called the “target distribution”). Under some technical conditions analogous to those we described in the countable case in Section 2.4, the Markov chain is known to converge to the stationary distribution regardless of the starting point. A sample from the posterior distribution can then be obtained by simulating from the process (i.e., repeatedly sampling from its transition kernel) for many time steps.</p>
<p>So how does one construct a Markov chain that has the target distribution as its stationary distribution? There is an enormous amount of research on this topic, and here we present just one possible approach, a special case of the Metropolis algorithm.</p>
<p><strong>Metropolis algorithm</strong> (with normal proposal distribution)</p>
<ol type="1">
<li><p>Start from <span class="math inline">\(x_0\)</span></p></li>
<li><p>For each iteration <span class="math inline">\(n = 1, 2, 3, \dots\)</span>,</p>
<ul>
<li><p>Propose a new value <span class="math inline">\(x^*\)</span>, drawn from the distribution <span class="math inline">\(N(x_{n-1}, \sigma^2)\)</span></p></li>
<li><p>Compute the acceptance ratio <span class="math display">\[
  \alpha = \min \left\{ 1,\ \frac{f(x^*)}{f(x_{n-1})} \right\}
\]</span></p></li>
<li><p>Choose the next value as <span class="math display">\[
  x_n =
  \begin{cases}
      x^* &amp; \text{with probability } \alpha \\
      x_{n-1} &amp; \text{with probability } 1 - \alpha
  \end{cases}
\]</span></p></li>
</ul></li>
</ol>
<p>This indeed defines a Markov chain, because the distribution of <span class="math inline">\(X_{n+1}\)</span> is determined by <span class="math inline">\(X_n\)</span> only, and it can be shown that <span class="math inline">\(f\)</span> is its stationary distribution. <a href="#fig-metropolis" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-metropolis</span></a> shows a histogram of values generated from the Metropolis algorithm, where <span class="math inline">\(f\)</span> is the probability density from the previous figure.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-metropolis" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-metropolis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02_markov_discrete_files/figure-html/fig-metropolis-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-metropolis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.17: Example probability density function that we would like to sample from.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="google-search" class="level3" data-number="2.8.2">
<h3 data-number="2.8.2" class="anchored" data-anchor-id="google-search"><span class="header-section-number">2.8.2</span> Google search</h3>
<p>The PageRank algorithm is a method for ranking web pages based on their importance and relevance to a particular search query, which was originally used by Google. Although the original paper by <span class="citation" data-cites="brin1998">Brin and Page (<a href="#ref-brin1998" role="doc-biblioref">1998</a>)</span> did not make that connection, the algorithm uses a Markov chain to model the behaviour of a hypothetical web surfer who randomly clicks on links from one page to another.</p>
<p>The basic idea is to find the proportion of time spent on different web pages, assuming that the surfer will randomly click on links until they reach a dead end (i.e., a page with no outgoing links). To model this behavior as a Markov chain, we can represent each web page as a state in the chain, and the links between pages determine its transition probabilities. Then, assuming that the resulting process is irreductible, its stationary distribution can be computed to find the long-term proportion of time spent on each page.</p>
<p>This approach is superior to simply counting how many pages link to a given website, because this would be ignoring the fact that those pages don’t all have the same weight. A link from a very popular page matters more, and this is captured by this random-surfer model.</p>
<p>As an example, we consider the web pages corresponding to the transition graph shown in <a href="#fig-graph-pagerank" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-graph-pagerank</span></a>. Each node is a web page, and each arrow is a link. Note that there are two absorbing states, E and F, which means that the process is not irreducible. To solve this problem, the common solution is to assume that the surfer randomly opens a new page (from all existing pages) when they run out of links to click. (This means that we will work with the chain that has outgoing links from E and F to every other page.)</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-graph-pagerank" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graph-pagerank-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-graph-pagerank">graph LR;
1((A))--&gt;2((B));
1--&gt;3((C));
1--&gt;6((F));
2--&gt;1;
2--&gt;4((D));
3--&gt;1;
3--&gt;5((E));
3--&gt;6;
4--&gt;2;
4--&gt;3;
4--&gt;6;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graph-pagerank-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.18: Example transition graph for six webpages, where the arrows are links.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The first step is to compute the transition probability matrix. The transition probability from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> is zero if there is no link, and it is one over the number of outgoing links from <span class="math inline">\(i\)</span> otherwise. Here, we start by defining an adjacency matrix, and then normalise the rows to get transition probabilities. Finally, we compute the stationary distribution using a result from Section 2.4.4.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the adjacency matrix</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span>  <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>               <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>               <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>               <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">nrow =</span> N, <span class="at">ncol =</span> N, <span class="at">byrow =</span> <span class="cn">TRUE</span>, </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">dimnames =</span> <span class="fu">list</span>(LETTERS[<span class="dv">1</span><span class="sc">:</span>N], LETTERS[<span class="dv">1</span><span class="sc">:</span>N]))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove sinks</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>A[<span class="fu">which</span>(<span class="fu">rowSums</span>(A) <span class="sc">==</span> <span class="dv">0</span>),] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the transition probability matrix</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> A <span class="sc">/</span> <span class="fu">rowSums</span>(A)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(P, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      A     B     C     D     E     F
A 0.000 0.333 0.333 0.000 0.000 0.333
B 0.500 0.000 0.000 0.500 0.000 0.000
C 0.333 0.000 0.000 0.000 0.333 0.333
D 0.000 0.333 0.333 0.000 0.000 0.333
E 0.167 0.167 0.167 0.167 0.167 0.167
F 0.167 0.167 0.167 0.167 0.167 0.167</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the PageRank scores</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">colSums</span>(<span class="fu">solve</span>(<span class="fu">diag</span>(N) <span class="sc">-</span> P <span class="sc">+</span> <span class="fu">matrix</span>(<span class="dv">1</span>, N, N)))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(v, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    A     B     C     D     E     F 
0.194 0.167 0.167 0.139 0.111 0.222 </code></pre>
</div>
</div>
<p>A person randomly clicking on the links on these six pages will therefore spend roughly 19% of their time on page A, 17% on page B, and so on. These proportions can be used as weights to rank the pages in terms of their overall importance.</p>
</section>
<section id="n-gram-models-predictive-text" class="level3" data-number="2.8.3">
<h3 data-number="2.8.3" class="anchored" data-anchor-id="n-gram-models-predictive-text"><span class="header-section-number">2.8.3</span> <span class="math inline">\(N\)</span>-gram models (predictive text)</h3>
<p>Say we want to predict the next word that someone will type, based on what they have typed so far (e.g., for word suggestions in a text message). If we think of each word as a state, the text can be modelled as a Markov chain, and the transition probabilities tell us how likely it is for any two words to be next to each other. For example, we might expect <span class="math display">\[
\Pr(X_{n+1} = \text{sky} \mid X_{n} = \text{blue})
\]</span> to be larger than <span class="math display">\[
\Pr(X_{n+1} = \text{tree} \mid X_{n} = \text{blue})
\]</span> If we just typed “blue”, the model might suggest “sky” (rather than “tree”) as the next word.</p>
<p>More realistically, we might think of the text as a higher-order Markov chain, where each word depends on several previous words. This greatly increases the number of transition probabilities, but improves the quality of the text that the model can generate. For example, in a third-order Markov chain, each transition probability looks something like <span class="math display">\[
\begin{aligned}
&amp; \Pr(X_{n+1} = \text{eat} \mid X_{n} = \text{to}, X_{n-1} = \text{want}, X_{n-2} = \text{I}) \\
&amp; \Pr(X_{n+1} = \text{travel} \mid X_{n} = \text{to}, X_{n-1} = \text{want}, X_{n-2} = \text{I}) \\
&amp; \dots
\end{aligned}
\]</span></p>
<p>As we discussed in an earlier section, the higher-order Markov chain can also be thought of as a (first-order) Markov chain on an expanded state space. In this case, each state is a sequence of <span class="math inline">\(N\)</span> words, which is called an <span class="math inline">\(N\)</span>-gram.</p>
<p>Here are some examples of text generated from a first-order, second-order, and third-order model, where the corpus used for estimation was the full text of all Sherlock Holmes novels.</p>
<p><strong>First-order model:</strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>I followed them on to the operation will receive.
I came alive solely through the risk in the
reputation among the major. He ran so terrible
event which we on the chairman of the thief who
loathed every excuse to explain them." "He was
sufficiently strange expedition? I thrust into it,
but when the woods. He repeated blows of one who
had best of no longer the Boots which I was one
connected with such complaints, which I can." At
present knowledge; and child, this chain, and by
the body lay," said Holmes. "It could frighten old
chap, he could not
 ...</code></pre>
</div>
</div>
<p><strong>Second-order model:</strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>I am never precipitate in my face. "And now as to
the yard, though I have no idea of a paved
causeway, and at once and forever." In mere size
and shape, and were smoking our pipes in the world
and could not leave the horse until the time of
the Peak country, in which you sent from London
Bridge Station, and was already a sinister relish
the formidable letters which purport to come again
whenever she might very easily--I might almost
have been submitted to him. Here is the daughter
of a hot scent, and then halted in the State
 ...</code></pre>
</div>
</div>
<p><strong>Third-order model:</strong></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>I am sorry that he has a grievance," he said. "He
thinks that it was impossible for me to handle,
and I'll hear of it and pour melted lead into the
hole so as to be able to post me up. I showed him
the empty box. They had only just been drawn
straight when Lestrade's voice was heard in the
silence I heard a shocking story of how he had
sniffed his lips, and there were several closed
doors and a long mirror. Holmes went to the altar
with him with the bust some months ago. We ordered
three busts
 ...</code></pre>
</div>
</div>
<!-- =========================================== -->
<!-- ================= SECTION ================= -->
<!-- =========================================== -->
</section>
</section>
<section id="problems" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="problems"><span class="header-section-number">2.9</span> Problems</h2>
<ol type="1">
<li><p>For each situation described below, do you think that the Markov assumption is reasonable for the process <span class="math inline">\((X_n)\)</span>? Briefly explain your reasoning, possibly including additional assumptions you need to make for the Markov property to hold.</p>
<ol type="a">
<li><p>Let <span class="math inline">\(X_n\)</span> be the price of gas at your local gas pump on day <span class="math inline">\(n\)</span>.</p></li>
<li><p>You are playing roulette, a game where a ball is spun on a wheel with 36 slices, and you try to guess the slice where you think the ball will end up. Each time you guess correctly, you win $100; each time you guess incorrectly, you lose $3. Let <span class="math inline">\(Y_n\)</span> be the amount you gain or lose on the <span class="math inline">\(n\)</span>-th round, and define <span class="math display">\[
X_n =
\begin{cases}
   0 &amp; \text{if } n = 0, \\
   \sum_{i=1}^n Y_i &amp; \text{otherwise}.
\end{cases}
\]</span></p></li>
<li><p>Consider a professional chess game, and define <span class="math inline">\(X_n \in \{ 1, 2, \dots, 64\}\)</span> as the position of the black queen on the board after the <span class="math inline">\(n\)</span>-th round.</p></li>
<li><p>You are listening to music, and you are using the shuffle mode to play all songs on your phone in random order. Let <span class="math inline">\(X_n\)</span> be the number of songs by your favourite band that have been played after the <span class="math inline">\(n\)</span>-th play.</p></li>
<li><p>Let <span class="math inline">\(X_n\)</span> be the number of centenarians living in Canada on day <span class="math inline">\(n\)</span>.</p></li>
</ol></li>
<li><p>Let <span class="math inline">\((X_n)\)</span> be the discrete-time Markov process with state space <span class="math inline">\(\mathcal{S} = \{ 0, 1, 2, 3\}\)</span> and transition probability matrix <span class="math display">\[
  \boldsymbol{P} =
  \begin{pmatrix}
0.8 &amp; 0.1 &amp; 0 &amp; 0.1 \\
0.3 &amp; 0.5 &amp; 0 &amp; 0.2 \\
0 &amp; 0.3 &amp; 0.7 &amp; 0 \\
0.1 &amp; 0.1 &amp; 0.1 &amp; 0.7
  \end{pmatrix}.
\]</span></p>
<ol type="a">
<li><p>Write a sequence of 10 states that could have come from this process (i.e., a possible realisation of length 10).</p></li>
<li><p>Write a sequence of 10 states that could <em>not</em> have come from this process.</p></li>
<li><p>Draw the transition graph of the process.</p></li>
<li><p>Which of the following two sequences is most likely under the model described above? Justify your answer.</p>
<ul>
<li>0, 0, 0, 3, 2, 2</li>
<li>0, 1, 1, 1, 3, 3</li>
</ul></li>
</ol></li>
<li><p>Let <span class="math inline">\((X_n)\)</span> be a <span class="math inline">\(p\)</span>-th order Markov chain with <span class="math inline">\(N\)</span> states, and let <span class="math inline">\((\tilde{X}_n)\)</span> be the equivalent first-order Markov chain with expanded state space.</p>
<ol type="a">
<li><p>How many states does <span class="math inline">\((\tilde{X}_n)\)</span> have?</p></li>
<li><p>How many non-zero transition probabilities does <span class="math inline">\((\tilde{X}_n)\)</span> have, at most? You should present a proof or justification for any <span class="math inline">\(p\)</span> and <span class="math inline">\(N\)</span>.</p></li>
</ol></li>
<li><p>Consider the Markov process <span class="math inline">\((X_n)\)</span> defined in Question 2, and the process <span class="math inline">\((Y_n)\)</span> defined by <span class="math inline">\(Y_n = X_{2n}\)</span>. Write R code to simulate a realisation of length 10 from <span class="math inline">\((Y_n)\)</span>, assuming <span class="math inline">\(Y_0 = 0\)</span>.</p></li>
<li><p>Let <span class="math inline">\((X_n)\)</span> be the 2-state Markov chain with transition probability matrix <span class="math display">\[
  \boldsymbol{P} =
  \begin{pmatrix}
1 - p &amp; p \\
q &amp; 1 - q    
  \end{pmatrix},
\]</span> where <span class="math inline">\(p, q \in (0, 1)\)</span>.</p>
<ol type="a">
<li><p>Show that <span class="math inline">\(\boldsymbol{P} = \boldsymbol{VDV}^{-1}\)</span> where <span class="math display">\[
\boldsymbol{V} =
\begin{pmatrix}
   1 &amp; p \\
   1 &amp; -q
\end{pmatrix}
\quad\text{and}\quad
\boldsymbol{D} =
\begin{pmatrix}
   1 &amp; 0 \\
   0 &amp; 1 - p - q
\end{pmatrix}
\]</span></p></li>
<li><p>Use the previous result to write the <span class="math inline">\(n\)</span>-step transition probability matrix <span class="math inline">\(\boldsymbol{P}^{(n)}\)</span> in terms of <span class="math inline">\(\boldsymbol{V}\)</span> and <span class="math inline">\(\boldsymbol{D}\)</span>.</p></li>
<li><p>Find the general expression for <span class="math inline">\(\boldsymbol{P}^{(n)}\)</span> in terms of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>.</p></li>
<li><p>What is the limit of <span class="math inline">\(\boldsymbol{P}^{(n)}\)</span> as <span class="math inline">\(n \to \infty\)</span>? Comment on the long-term behaviour of the process.</p></li>
</ol></li>
<li><p>For each of the following Markov processes: (i) draw the transition graph, (ii) determine whether <span class="math inline">\(\lim_{n \to \infty} \Pr(X_n = i)\)</span> exists for all <span class="math inline">\(i \in \mathcal{S}\)</span>, and (iii) determine whether the process has a limiting distribution. You do not need to prove transience, positive recurrence, or null recurrence, but you should justify your reasoning if you use one of these properties (e.g., by drawing a parallel with an example from the lectures).</p>
<ol type="a">
<li><p>Let <span class="math inline">\((X_n)\)</span> be the Markov process with state space <span class="math inline">\(\mathcal{S} = \{ 0, 1, 2, 3, 4 \}\)</span> and transition probability matrix <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
   0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
   \frac{1}{2} &amp; 0 &amp; \frac{1}{2} &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
   0 &amp; 1 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
\]</span></p></li>
<li><p>Let <span class="math inline">\((X_n)\)</span> be the Markov process with state space <span class="math inline">\(\mathcal{S} = \{ 0, 1, 2, 3, 4 \}\)</span> and transition probability matrix <span class="math display">\[
\boldsymbol{P} =
\begin{pmatrix}
   0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
   \frac{1}{2} &amp; 0 &amp; \frac{1}{2} &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
   0 &amp; 1 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
\]</span></p></li>
<li><p>Let <span class="math inline">\((X_n)\)</span> be the Markov process with state space <span class="math inline">\(\mathcal{S} = \mathbb{N}\)</span>, where <span class="math inline">\(P_{02} = 1\)</span> and, for <span class="math inline">\(i \geq 1\)</span>, <span class="math display">\[
P_{ij} =
\begin{cases}
   \frac{1}{3} &amp; \text{if } j = i,\\
   \frac{1}{3} &amp; \text{if } j = i - 1 ,\\
   \frac{1}{3} &amp; \text{if } j = i + 2.
\end{cases}
\]</span></p></li>
<li><p>Let <span class="math inline">\((X_n)\)</span> be the Markov process with state space <span class="math inline">\(\mathcal{S} = \mathbb{N}\)</span>, where <span class="math inline">\(P_{01} = 1\)</span>, <span class="math inline">\(P_{12} = 1\)</span> and, for <span class="math inline">\(i \geq 2\)</span>, <span class="math display">\[
P_{ij} =
\begin{cases}
   \frac{1}{3} &amp; \text{if } j = i ,\\
   \frac{1}{3} &amp; \text{if } j = i - 2 ,\\
   \frac{1}{3} &amp; \text{if } j = i + 1.
\end{cases}
\]</span></p></li>
</ol></li>
<li><p>Let <span class="math inline">\((X_n)\)</span> be the 3-state Markov process with state space <span class="math inline">\(\mathcal{S} = \{ 0, 1, 2 \}\)</span> and transition probability matrix <span class="math display">\[
  \boldsymbol{P} =
  \begin{pmatrix}
1-p &amp; p &amp; 0 \\
0 &amp; 1 - q &amp; q \\
r &amp; 0 &amp; 1 - r
  \end{pmatrix},
\]</span> where <span class="math inline">\(p, q, r \in (0, 1)\)</span>.</p>
<p>In the long run, what proportion of time will the process spend in each state?</p></li>
<li><p>Consider a special case of the two-state Markov process described in Question 5, with <span class="math inline">\(p = 0.05\)</span> and <span class="math inline">\(q = 0.2\)</span>. Write R code to answer the following questions.</p>
<ol type="a">
<li><p>Compute <span class="math inline">\(P_{01}^{(n)}\)</span> and <span class="math inline">\(P_{10}^{(n)}\)</span> for <span class="math inline">\(n \in \{ 1, 2, 3, \dots, 19, 20\}\)</span>.</p></li>
<li><p>Create one graph showing how <span class="math inline">\(P_{01}^{(n)}\)</span> and <span class="math inline">\(P_{10}^{(n)}\)</span> change with <span class="math inline">\(n\)</span>. This should be shown with two lines on the same panel, and the y axis should range from 0 to 1.</p></li>
<li><p>Does the graph support your answer for Question 5d? Justify your answer.</p></li>
</ol></li>
<li><p>Consider the Markov process <span class="math inline">\((X_n)\)</span> with state space <span class="math inline">\(\{ 0, 1, 2, 3, 4, 5, 6\}\)</span> and transition probability matrix <span class="math display">\[
  \boldsymbol{P} =
  \begin{pmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0.9 &amp; 0 &amp; 0.1 &amp; 0 &amp; 0 \\
0.6 &amp; 0 &amp; 0 &amp; 0.2 &amp; 0.2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.5 &amp; 0.5 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0
  \end{pmatrix}.
\]</span></p>
<ol type="a">
<li><p>Given <span class="math inline">\(X_0 = 6\)</span>, does the chain have a limiting distribution? If so, what is it?</p></li>
<li><p>Given <span class="math inline">\(X_0 = 6\)</span>, what is the long-term proportion of time that the chain will spend in each state?</p></li>
</ol></li>
<li><p>Consider the Markov chain with transition graph given in <a href="#fig-markov-prob1" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-prob1</span></a>. What is the expected hitting time of state 0, starting from each of the other three states?</p></li>
</ol>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-markov-prob1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-markov-prob1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-markov-prob1">graph LR;
1((1)) --&gt;|1/3| 0((0))
0 --&gt;|1| 2((2))
1 --&gt;|2/3| 2
2 --&gt;|1| 3((3))
3 --&gt;|1| 1
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-markov-prob1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.19: Markov chain for problem 10
</figcaption>
</figure>
</div>
</div>
</div>
<ol start="11" type="1">
<li><p>Consider the Markov chain with transition graph given in <a href="#fig-markov-prob2" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-markov-prob2</span></a>.</p>
<ol type="a">
<li><p>What is the probability that the process reaches state 3, if it starts in state 0?</p></li>
<li><p>What is the expected hitting time of state 5, if the process starts in state 0?</p></li>
</ol></li>
</ol>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-markov-prob2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-markov-prob2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-markov-prob2">graph LR;
0((0)) --&gt;|0.9| 1((1))
1 --&gt;|0.7| 2((2))
2 --&gt;|0.5| 3((3))
3 --&gt;|0.3| 4((4))
0 --&gt;|0.1| 5((5))
1 --&gt;|0.3| 5
2 --&gt;|0.5| 5
3 --&gt;|0.7| 5
4 --&gt;|1| 5
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-markov-prob2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.20: Markov chain for problem 11
</figcaption>
</figure>
</div>
</div>
</div>
<ol start="12" type="1">
<li><p>The original PageRank algorithm is based on the assumption that a person browsing the internet (the “surfer”) would click a link uniformly at random from all the links available on the page they are visiting (and repeat). This can be viewed as a Markov chain <span class="math inline">\((X_n)\)</span>, where <span class="math inline">\(X_n\)</span> is the <span class="math inline">\(n\)</span>-th webpage visited by the random surfer. Under some assumptions, the stationary distribution of the chain gives the long-term proportion of time spent on each webpage (a convenient metric to rank webpages). One problem is that the network of all webpages does not define an irreducible Markov chain, because some pages include no links (absorbing state), and some pages are not linked from anywhere.</p>
<p>A proposed solution is to modify the chain slightly so that, at each time step, there is a probability <span class="math inline">\(q\)</span> to click one of the links on the current page, and a probability <span class="math inline">\(1 - q\)</span> to choose a webpage at random from all existing webpages with equal probability (regardless of links). This is analogous to a surfer closing the webpage they are currently visiting, and opening a new random webpage. We call the modified Markov chain <span class="math inline">\((Y_n)\)</span>.</p>
<ol type="a">
<li><p>Is <span class="math inline">\((Y_n)\)</span> irreducible? Justify your answer.</p></li>
<li><p>If we denote as <span class="math inline">\(\boldsymbol{P}\)</span> the transition probability matrix of <span class="math inline">\((X_n)\)</span>, what is the transition probability matrix of <span class="math inline">\((Y_n)\)</span>?</p></li>
<li><p>Can you think of a computational downside to the proposed solution?</p></li>
</ol></li>
</ol>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-brin1998" class="csl-entry" role="listitem">
Brin, Sergey, and Lawrence Page. 1998. <span>“The Anatomy of a Large-Scale Hypertextual Web Search Engine.”</span> <em>Computer <span>Networks</span> and <span>ISDN</span> <span>Systems</span></em> 30 (1-7): 107–17.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01_background.html" class="pagination-link" aria-label="Background">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Background</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03_poisson.html" class="pagination-link" aria-label="Poisson processes">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Poisson processes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>